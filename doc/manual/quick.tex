The easiest way to get started
is to use the \nowdb\ docker containing
the database server.

\comment{more instructions of how to get it $\dots$}

The docker does not start the database by itself.
You need to start it explicitly. There is a script
called \tech{nowstart.sh} in the root of the docker
that does that.
(You may want to adapt this script to your specific needs!)
Here is a reasonable way to start the docker:

\code{
docker run --rm -p 55505:55505 $\backslash$\\
\hspace*{2cm} -v /opt/dbs:/dbs -v /var/log:/log $\backslash$\\
\hspace*{2cm} -d nowdbdocker /bin/bash -c "/nowstart.sh"
}

The docker command is \term{run}.
It create a docker container and starts it.
We add a number of parameters:
\begin{itemize}
\item \tech{--rm}
instructs the docker daemon to remove
the docker immediately after it will have been stopped.

\item \tech{-p 55505:55505} binds the host port \term{55505}
to the same docker port.

\item \tech{-v} maps a host path
(namely, the path \tech{/opt/dbs}) to a docker path
(namely, to \tech{/dbs}) and
the path \tech{/var/log} to \tech{/log}.

\item \tech{-d} means the docker runs in the background
(\term{detached}).

\item \tech{nowdbdocker} is the name of the docker.

\item \tech{/bin/bash} is the command to be executed
within the docker; \tech{-c} passes a command
to be executed,
namely \tech{/nowstart.sh},
to the \tech{bash}.
\end{itemize}

The script \tech{nowstart.sh}, contains the
instructions to start the nowdb daemon (whose name is
\tech{nowdbd}).

It first sets some environment variables:

\code{
export LD\_LIBRARY\_PATH=/lib:/usr/lib:/usr/local/lib
}

This sets the search path for shared libraries.

\code{
export PYTHONPATH=/pynow:\$PYTHONPATH
}

This sets the search path for Python modules
(we will discuss that later).

Then the script starts the daemon iself:

\code{
nowdbd -b /dbs -y 2>/log/nowdbd.log
}

The script passes two options to the daemon:
the base directory where all databases
managed by this daemon live (\tech{-b /dbs})
and the \tech{-y} switch, which activates
server-side Python support.

Now the daemon is listening to port 55505
and ready to respond to database requests.
The daemann starts by printing a welcome banner
to standard output:

\begingroup
\small
\begin{verbatim}
+---------------------------------------------------------------+ 
 
  UTC 2018-10-09T12:03:21.631000000
 
  The server is ready
 
    - with python support enabled
 
+---------------------------------------------------------------+ 
  nnnn   nnnn          nnnn    nnnnnn       nnnnnn       nnnnnn  
    wi  i   wi       i      i    iw           iw           er   
    wi i     wi     n        n    iw         e  wi        e    
    wii      wi    wi        iw    iw       e    wi      e        
    wi       wi    wi        iw     iw     e      wi    e        
    wi       wi     n        n       iw   e        wi  e        
    wi       wi      i      i         iw e          wie          
   nnnn     nnnn       nnnn             n            n            
+---------------------------------------------------------------+ 

connections: 128
port       : 55505
domain     : any
\end{verbatim}
\endgroup

Here are some more options provided by
the \term{nowdbd} tool:

\begin{itemize}
\item \tech{-b} The base directory,
where databases are stored.
(default is the current working directory).

\item \tech{-s} the binding domain, default:
any. If set to a host or a domain, the server
will accept only connections from that host
or domain. Example: \tech{-s localhost} does
only accept connections from the server.
The host or domain can be given as name (\term{localhost})
or as \acronym{ip} address (\tech{127.0.0.1});

\item \tech{-p} the port the server to which
the server will listen; default is 55505,
but any other (free) port may be used.
 
\item \tech{-c} size of the connection pool.
If the argument is 0, the connection pool grows
indefinitely; otherwise, for \tech{-c n},
$n$ being a positive integer,
the server will create a connection pool
up to $n$ sessions and then refuse to accept
more connections. The default pool size is 128;
\comment{Notice that due to a memory leak
in the Python interpreter, there is no way
to instruct the server to accept indefinitely
many connections, but to only maintain $n$
in the pool.}

\item \tech{-q} runs in quiet mode
(\ie\ no debug messages are printed to standard error);
\item \tech{-n} does not print the starting banner;
\item \tech{-y} activate server-side Python support;
\item \tech{-l} activates server-side Lua support;
\item \tech{-V} prints version information to standard output;
\item \tech{-?} or \tech{-h} prints a help message to standard output.
\end{itemize}

Once \term{nowdbd} is running, we can connect to pass
queries to the server. There is a tool to do this from the commandline
called \term{nowclient}.

\comment{how to install the client tools???}

To send a query to the server
the client tool can be used like this:

\code{
nowclient -d retail -Q "select count(*) from sales where customer=12345"
}

In this form, the client will try to connect to a server
running on the same host and listening to port 55505.
Furthermore, it will request to use the database \term{retail}
and send the query indicated by the \tech{-Q} parameter.

If the connection is successful, the client will print some
processing information to standard error and the query result
to standard output, \eg:

\code{
executing "use retail" \\
OK \\
executing "select count(*) from sales where customer=12345" \\
59
}

The option \tech{-q} would suppress the processing information.
We would then only see:

\code{
59
}

Here are more options supported by the client tool:
\begin{itemize}
\item \tech{-s} 
The server address or name, \eg\ \tech{myserver.mydomain.org} or
\tech{127.0.0.1}. Default is \tech{127.0.0.1};

\item \tech{-p}
the port to which the database is listening. Default: 55505;

\item \tech{-d}
the database to which we want to connect. 
Default: no database at all, which means
that we cannot send queries without naming a database.
Below we will look at alternatives to using this parameter;

\item \tech{-Q}
the query we want the database to process;

\item \tech{-t}
print some (server-side) timing information to standard error;

\item \tech{-q}
quiet mode: don't print processing information to standard error.

\item \tech{-V} prints version information to standard output;
\item \tech{-?} or \tech{-h} prints a help message to standard output.
\end{itemize}

The client tool is able to read from standard input;
this way, more than one query can be processed at a time.
The following command processes the same query as the one above,
but uses standard input instead of the options \tech{-d} and \tech{-Q}:

\code{
echo "use retail;select count(*) from sales where customer=12345;" $| \backslash$ \\
\hspace*{2cm} nowclient
}

Notice that, using standard input,
we need to terminate single \sql\ statements
by a semicolon. This is even true for the last statement.
Leaving the semicolon out would lead to an error.

Of course, we can do much more useful things than just
getting rid of the options.
The main point of reading from standard input is
that we can put \sql\ statements into a file and
cat it to nowclient. A useful example may be:

\begin{sqlcode}
\begin{lstlisting}
drop schema retail if exists;
create schema retail; use retail;

create large table sales set stress=constant;
create table statistics;

create medium index idx_sales_eds on sales (edge, destin);
create index idx_sales_eor on sales (edge, origin);

create type product (
        prod_key uint primary key,
        prod_desc text,
        prod_price float
);
create type client (
        client_key uint primary key,
        client_name text
);
create edge buys (
        origin client,
        dest product,
        weight uint,
        weight2 float
);
load '/opt/data/products.csv' into vertex use header as product;
load '/opt/data/clients.csv' into vertex use header as client;
load '/opt/data/sales.csv' into sales;
\end{lstlisting}
\end{sqlcode}

Let's assume we had this code in a file called
\tech{create\_retail.sql}; then we could
send it to the nowdbclient:

\code{
cat create\_retail.sql | nowclient
}

which would create the retail database.

The script shows some of the peculiarities of \nowdb.
The beginning is quite regular \sql:

\begin{sqlcode}
\begin{lstlisting}
drop schema retail if exists;
create schema retail; use retail;
\end{lstlisting}
\end{sqlcode}

The first line drops the database retail,
\ie\ it removes all its data physically
from disk. The \term{if exists} part
is included to avoid an error 
(and hence the termination of the script)
in the case
the database does indeed exist.

The keyword \term{schema} is intended to 
make the \nowdb\ \sql\ dialect more
approachable for people with experience
in other \sql\ dialects. Instead of
\term{schema}, one could also say
\term{database}. 
Yet another way to say the same thing
in \nowdb\ is \term{scope}, which
means the scope in which a set of
vertices is valid.

In the second line the schema
(or database or scope) `retail'
is created. Since we have dropped it
if it already had existed before,
this statement will not cause
an error when the database already exists.
An alternative way of doing things is:

\begin{sqlcode}
\begin{lstlisting}
create schema retail if not exists;
\end{lstlisting}
\end{sqlcode}

In that case, we would maintain the database
if it exists already. The script, however,
would continue, because no error would occur.

The third statement (still in the second line)
instructs the \nowdb\ to use the newly created
schema `retail' in everything that follows.

The next line is a bit uncommon:

\begin{sqlcode}
\begin{lstlisting}
create large table sales set stress=constant;
\end{lstlisting}
\end{sqlcode}

The statement creates a table called `sales';
however, since \nowdb\ is not a relational,
but a graph database, there are no 
`tables' with the meaning of that term
in the relational world.
In fact, tables are just units of storage 
for edges. \comment{A better naming convention
would probably be `tablespace'.}

The statement explicitly says that we
want a \emph{large} table and that there
will be \emph{constant} stress (\ie\ ingestion load
on that table). The \term{create table} statement,
hence, is much more oriented to storage
and processing details than to the logical structure
of the table (as it would be in a relational database).

A similar is true for the index creation:

\begin{sqlcode}
\begin{lstlisting}
create medium index idx_sales_eds on sales (edge, destin);
\end{lstlisting}
\end{sqlcode}

This statement creates a \emph{medium} index on table sales
with the fields `edge' and `destination' as index keys.
Index sizing is a difficult topic and will be discussed
in chapter \ref{chpt_sizing}.
As a rule of thumb, it is almost always better to assume
small sizing. Large indices have large storage nodes
and those nodes are very often read from and written
to disk. Only when we know that our index will have
many keys and many data points per key, it is advisable
to use index sizing. The default sizing (used in the next
line) indicatively is \term{small}.

The next two blocks of code create the types
`product' and `client'. This should look a lot like
what you probably know as \term{tables}. Indeed types
are vertices stored as \term{column-oriented} tables.
\comment{Well, not entirely true at the moment.
In fact, completely column-oriented tables would cause
some more work in the underlying mechanisms. Anyway,
that is the direction to go and sooner or later
the statement will be true $\dots$}

Notice that every type needs a primary key that consists
of one attribute.

The attribute types are \term{uint}, \term{float} and \term{text}.
The first is a 64bit unsigned integer;
the second is a 64bit floating point number
(a.k.a. \term{double} in languages like C);
\term{text} is a string of up to 255 \acronym{utf8} characters.
For more information on \sql\ types, please refer to chapter
\ref{chpt_sql}.

The next block defines an edge. Remember
that edges, contrary to vertices, have a fixed layout.
However, we can define the types of the fields of an edge
and we can rename them. \comment{Renaming is not yet possible.}
Here, we define four of the fields of the edge.
The origin has type \term{client} (which we have defined just above)
and the destination has type \term{product}.
The edge has two weights: one is uint and the other float.

Finally, the script loads data from three different \acronym{csv}s
into the database. Loaders have the same effect as the \sql\
\term{insert} statement, but are much more efficient.
The drawback of \term{insert} is that each statement
needs the whole cycle of \sql\ parsing and execution,
while loaders only need one cycle. Since a data source
can contain millions or even billions of rows,
loading is way more efficient than inserting in most cases.

The first two \acronym{csv}s in the script contain vertices.
As such they need to have a header and we need to instruct
the loader of how to interpret the data (\tech{use header as client}).
Since edges always have the same format, they don't need a header
and we do not give any instructions of how to interpret the data.

\acronym{csv} is only one format supported by the database loader.
There are many more and the loader even allows users to define
their own (binary) formats using Apache Avro. Loading binary data 
can be much faster than loading textual formats such as \acronym{csv}.
Using a serialisation system like Avro eases
interoperability of the database with other
external systems and applications and it
significantly eases version management should data formats
change over time (what they always do!). 
For more details on data loaders, please refer to chapters
\ref{chpt_sql} and \ref{chpt_loader}.

The alternative to loading data is, of course, the conventional
\term{insert} statement:

\begin{sqlcode}
\begin{lstlisting}
insert into client(9000001, 'Popeye the Sailor');
insert into product(100001, 'Spinach, 450g net', 1.99);
\end{lstlisting}
\end{sqlcode}

These two statements insert a client and a product respectively.
We can also name the attributes explicitly, like:

\begin{sqlcode}
\begin{lstlisting}
insert into product(prod_key, prod_desc, prod_price)
              (100002, 'Candy Cigarettes, 20', 2.49);
\end{lstlisting}
\end{sqlcode}

Now we insert a bunch of edges:

\begin{sqlcode}
\begin{lstlisting}
insert into sales (edge, origin, destin, timestamp, weight, weight2)
           ('buys', 9000001, 100001, '1929-01-17T09:35:12', 1, 1.99)
insert into sales (edge, origin, destin, timestamp, weight, weight2)
           ('buys', 9000001, 100001, '1929-01-19T10:15:01', 2, 3.98)
insert into sales (edge, origin, destin, timestamp, weight, weight2)
           ('buys', 9000001, 100001, '1929-01-20T17:12:55', 3, 5.97)
insert into sales (edge, origin, destin, timestamp, weight, weight2)
           ('buys', 9000001, 100001, '1929-01-22T08:27:32', 1, 1.99)
insert into sales (edge, origin, destin, timestamp, weight, weight2)
           ('buys', 9000001, 100001, '1929-01-25T12:09:59', 1, 1.99)
insert into sales (edge, origin, destin, timestamp, weight, weight2)
           ('buys', 9000001, 100001, '1929-01-26T21:19:44', 2, 3.98)
insert into sales (edge, origin, destin, timestamp, weight, weight2)
           ('buys', 9000001, 100002, '1929-01-22T08:27:51', 1, 2.49)
\end{lstlisting}
\end{sqlcode}

Worth noticing here is the time format,
which follows \acronym{iso}-8601.

The format is
\begin{itemize}
\item 4 digits for the year and hyphen
\item 2 digits for the month and hyphen
\item 2 digits for the day of month
\item `T' to mark the beginning of the time section
\item 2 digits for the hour and colon
\item 2 digits for the minute and colon
\item 2 digits for the second and,
\item if finer grain is necessary,
a dot followed by up to 9 digits
for the nanosecond
\end{itemize}

This format can be used anywhere in \nowdb\ \sql.
However, it is also possible to define custom
date and time formats. How to do this is discussed
in chapter \ref{chpt_sql}.

Now that we have inserted some data
into our database, we are able to perform selects, \eg:

\code{
nowclient -d retail -Q
"select count(*) from sales $\backslash$ \\
\hspace*{4.7cm} where edge='buys' $\backslash$ \\
\hspace*{4.7cm} and origin=9000001"
}

which would give us 7 and would count Popeye's visits to the supermarket.
But we could also count how often Spinach was bought:

\begin{sqlcode}
\begin{lstlisting}
select count(*) from sales
 where edge='buys'
   and destin=100001
\end{lstlisting}
\end{sqlcode}

which would give us 6.
Or we could ask how much Popeye bought and paid per type of product:

\begin{sqlcode}
\begin{lstlisting}
select edge, destin, count(*), sum(weight), sum(weight2)
  from sales 
 where edge='buys' 
   and origin=9000001 
 group by edge, destin
\end{lstlisting}
\end{sqlcode}

which would give us:

\begin{verbatim}
buys;100001;6;10;19.9000
buys;100002;1;1;2.4900
\end{verbatim}

Two aspects are worth mentioning here:
First, the statement with the group clause above
would use the index we defined on sales
with fields $(edge, destin)$.
To force the use of an index in a group clause
there must be an exact match between the fields
in the group clause and the index; also the order
of keys in the index and in the clause must be the same.
For instance, \code{group by destin, edge} would
not have forced the use of the index.
Whether using an index or not for grouping
is an advantage 
depends on many factors.
For a detailed discussion of indices and
optimising queries please refer to chapter \ref{chpt_opt}.

Second, you may have noticed that the output
of the client tool does not resemble the classical
pretty-printed output produced by most database
clients today. The advantage of such output is
that it is easier for humans to read.
The \acronym{csv}-like output shown above, however,
is easer to read for machines, especially
when you want to send the output through pipes
to other programs, like this:

\code{
nowclient -q -d retail -Q "select * from sales" | cut -d";" -f2 | $\dots$
}

On the other hand, there are tools that
produce more readable output from \acronym{csv} input,
such as \tech{csvlook} from the \tech{csvkit} package.\footnote{Have
a look at
\url{https://github.com/jeroenjanssens/data-science-at-the-command-line}}
To obtain a traditional pretty-printed output you could do:

\code{
cat query.sql | nowclient | csvformat -d";" | $\backslash$ \\
    header -a 'edge,product,count,quantity,price' | csvlook
}

and would obtain for the grouping query used above:

\begin{verbatim}
|----------+---------+-------+----------+---------|
|  edge    | product | count | quantity | price   |
|----------+---------+-------+----------+---------|
|  buys    | 100001  | 6     | 10       | 19.9000 |
|  buys    | 100002  | 1     | 1        | 2.4900  |
|----------+---------+-------+----------+---------|
\end{verbatim}

Here is a more typical time series query illustrating
the advantage of the pretty printer:

\begin{sqlcode}
\begin{lstlisting}
select destin, timestamp, weight, weight2
  from sales 
 where edge='buys' 
   and origin=9000001 
 order by timestamp
\end{lstlisting}
\end{sqlcode}

which, with the same technique used above, shows:

\begin{verbatim}
|----------+---------------------+----------+---------|
|  product | timestamp           | quantity | price   |
|----------+---------------------+----------+---------|
|  100001  | 1929-01-17T09:35:12 | 1        | 1.9900  |
|  100001  | 1929-01-19T10:15:01 | 2        | 3.9800  |
|  100001  | 1929-01-20T17:12:55 | 3        | 5.9700  |
|  100001  | 1929-01-22T08:27:32 | 1        | 1.9900  |
|  100002  | 1929-01-22T08:27:51 | 1        | 2.4900  |
|  100001  | 1929-01-25T12:09:59 | 1        | 1.9900  |
|  100001  | 1929-01-26T21:19:44 | 2        | 3.9800  |
|----------+---------------------+----------+---------|
\end{verbatim}

We can also select from vertices,
but instead of a table, we use the type:

\begin{sqlcode}
\begin{lstlisting}
select prod_price from product
 where prod_key = 100001;
\end{lstlisting}
\end{sqlcode}

would give
\begin{verbatim}
1.99
\end{verbatim}

and

\begin{sqlcode}
\begin{lstlisting}
select client_name from client
 where client_key = 9000001;
\end{lstlisting}
\end{sqlcode}

would give
\begin{verbatim}
Popeye the Sailor
\end{verbatim}

\comment{
This is as it should be.
Unfortunately, there are some inconsistencies stemming 
from the fact that vertices are column-oriented.
The syntax is currently `select field from vertex as type',
which is ugly and pointless.
Even worse, grouping, ordering and aggregates
are not yet implemented for vertices.
That must be corrected \acronym{asap}.
}

Much more typical for \nowdb, however,
is to use vertices together with edges. Edges connect vertices
and can therefore be seen as the `relations' in \nowdb.
What we typically want is either find the vertex
at the other end of the edge (\eg\ find the destination
for a given origin) or to look at edges with the attributes
of the vertices added to them.

Both patterns are, in \sql, instances of \term{joins}.
An instance of the first pattern would be:

\begin{sqlcode}
\begin{lstlisting}
select timestamp, prod_desc, prod_price
  from sales join product
    on prod_key = destin
 where edge = 'buys'
   and origin = 9000001;
\end{lstlisting}
\end{sqlcode}

\begin{verbatim}
|---------------------+----------------------+---------|
| timestamp           | product              | price   |
|---------------------+----------------------+---------|
| 1929-01-17T09:35:12 | Spinach, 450g net    | 1.9900  |
| 1929-01-19T10:15:01 | Spinach, 450g net    | 1.9900  |
| 1929-01-20T17:12:55 | Spinach, 450g net    | 1.9900  |
| 1929-01-22T08:27:32 | Spinach, 450g net    | 1.9900  |
| 1929-01-25T12:09:59 | Spinach, 450g net    | 1.9900  |
| 1929-01-26T21:19:44 | Spinach, 450g net    | 1.9900  |
| 1929-01-22T08:27:51 | Candy Cigarettes, 20 | 2.4900  |
|---------------------+----------------------+---------|
\end{verbatim}

Note the difference in the price column.
In the previous query we used \term{weight2} of sales,
which (as you may have realised)
is the multiplication of the product price and the value
in \term{weight}, which, in its turn,
represents the number of items.
Here, however, we use the value in \term{prod\_price}
which is the base price of one unit of that product.

We can, of course, combine joins with grouping:

\begin{sqlcode}
\begin{lstlisting}
select edge, destin, count(*), sum(prod_price)
  from sales join product
    on prod_key = destin
 where edge = 'buys'
   and origin = 9000001
 group by edge, destin;
\end{lstlisting}
\end{sqlcode}

\begin{verbatim}
|----------+---------+-------+---------|
|  edge    | product | count | price   |
|----------+---------+-------+---------|
|  buys    | 100001  | 6     | 11.9400 |
|  buys    | 100002  | 1     | 2.4900  |
|----------+---------+-------+---------|
\end{verbatim}

Note again the sum of the price which, here,
is just the sum of the base price per unit of the product.

The point about the first joining pattern
is that it joins only one of the two vertices
with the edge. The second pattern is somewhat more complex:
it joins both vertices, like:

\begin{sqlcode}
\begin{lstlisting}
select timestamp, prod_desc, prod_price, client_name
  from sales
  join product
    on prod_key = destin
  join client
    on client_key = origin
 where edge = 'buys'
   and origin = 9000001;
\end{lstlisting}
\end{sqlcode}

The result of this query would be:

\begin{verbatim}
|---------------------+----------------------+---------+-------------------|
| timestamp           | product              | price   | client            |
|---------------------+----------------------+---------+-------------------|
| 1929-01-17T09:35:12 | Spinach, 450g net    | 1.9900  | Popeye the Sailor |
| 1929-01-19T10:15:01 | Spinach, 450g net    | 1.9900  | Popeye the Sailor |
| 1929-01-20T17:12:55 | Spinach, 450g net    | 1.9900  | Popeye the Sailor |
| 1929-01-22T08:27:32 | Spinach, 450g net    | 1.9900  | Popeye the Sailor |
| 1929-01-25T12:09:59 | Spinach, 450g net    | 1.9900  | Popeye the Sailor |
| 1929-01-26T21:19:44 | Spinach, 450g net    | 1.9900  | Popeye the Sailor |
| 1929-01-22T08:27:51 | Candy Cigarettes, 20 | 2.4900  | Popeye the Sailor |
|---------------------+----------------------+---------+-------------------|
\end{verbatim}

\comment{
Unfortunately, joins are not yet available :-(
}

Edges have one more important field that we have not yet discussed,
namely the \term{label}. The \term{label} is intended to create a connection
between edges that are inherently related. In our example, we see
that at one day Popeye bought two different things:
spinach and candy cigarettes.
That was at Jan, 22.

These two edges, hence, relate to the same visit at the supermarket.
We could link these edges using a label. The edge would then be created
as, for instance:

\begin{sqlcode}
\begin{lstlisting}
create edge buys (
        origin client,
        dest product,
        label text,
        weight uint,
        weight2 float
);
\end{lstlisting}
\end{sqlcode}

The label text would correspond to a token generated by
the cashpoint when a customer starts the check-out.
We would then insert edges as follows:

\begin{sqlcode}
\begin{lstlisting}
insert into sales (edge, origin, destin, timestamp,
                            label, weight, weight2)
   ('buys', 9000001, 100001, '1929-01-22T08:27:32',
                      'tx-19290122082731', 1, 1.99)
insert into sales (edge, origin, destin, timestamp,
                            label, weight, weight2)
   ('buys', 9000001, 100002, '1929-01-22T08:27:51', 
                      'tx-19290122082731', 1, 2.49)
\end{lstlisting}
\end{sqlcode}

We could now select edges according to this label:

\begin{sqlcode}
\begin{lstlisting}
select destin, timestamp, weight, weight2
  from sales 
 where label = 'tx-19290122082731'
 order by timestamp
\end{lstlisting}
\end{sqlcode}

This query would result in:

\begin{verbatim}
|----------+---------------------+----------+---------|
|  product | timestamp           | quantity | price   |
|----------+---------------------+----------+---------|
|  100001  | 1929-01-22T08:27:32 | 1        | 1.9900  |
|  100002  | 1929-01-22T08:27:51 | 1        | 2.4900  |
|----------+---------------------+----------+---------|
\end{verbatim}

Until here we always used the client \emph{tool}
to perform queries.
That is certainly an important use case.
Much more typical, however, is to develop application code
that needs a client \acronym{api} to connect to the database.

\nowdb\ comes with a native client \acronym{api}
that is available in different languages, among others
C, \CC, \csharp, Go, Python and Lua.
\comment{Only Python is currently available.
There is a low-level \acronym{api} for C,
but that is not for developing applications,
but for developing \acronym{api}s.}
Here, we will have a quick look at the Python \acronym{api}.

The module implementing the Python \nowdb\ \acronym{api}
is called \term{now.py} and must be imported into the client program.
For the Python interpreter to find this module,
it must be in a directory in the \acronym{pythonpath}.
There may be different ideas on how to install python modules.
The \nowdb\ installation will copy all \nowdb-related modules
to one specific folder and add this folder to the
\acronym{pythonpath}. But you also may install
the \nowdb\ Python \acronym{api} using \term{pip}.
Then, everything is handled by the Python environment
and you don't need to care about these things.
For more details on installation, please refer
to chapter \ref{chpt_install}.

Anyway, here is a simple Python program:

\begin{python}
\begin{lstlisting}
import now

with now.Connection("localhost", "55505", None, None) as c:
   with c.execute("use retail") as r:
       if not r.ok():
          print "cannot use retail: %s" % r.details()
          exit(1)

   with c.execute("select count(*), sum(weight), avg(weight) \
                     from sales where edge='buys'") as cur:
       if not cur.ok():
          print "ERROR: %s" % cur.details()
          exit(1)
       for row in cur:
           print "count: %d, sum: %d, avg: %.2f" %
                 (row.field(0), row.field(1), row.field(2))
\end{lstlisting}
\end{python}

The program first creates a \term{Connection}
to a database listening on port 55505 on `localhost'.
It then executes a \term{use} statement on this connection
to indicate the database towards which
the following statements are directed.

The result of a \term{use} statement is a \term{status}.
A status is either \acronym{ok} or an error.
The details of the can be obtained by means of the method
\term{details()}, which returns a string.
Results also support the method \term{code()},
which would return a numeric error code.
It is often useful to know the error code to decide
what to do programmatically (abandon the program,
retry, try something else, \etc)

Results are also resource managers. That means
that they can be used inside a \term{with} statement.
\term{with} assures that all resources (in this case
the result) are freed before the control leaves
the \term{with}-block even if an exception is raised.

The program then executes a query.
This time the execution returns a \term{cursor} (`cur').
The program checks whether the cursor is
in a good state. The statement may have
failed on the server side. The cursor would
then be in a state that is not \acronym{ok}.
In that case, the program prints the error details
and exits with return code 1.

Otherwise, if the result was fine,
it iterates over the cursor
printing for each row the fields 1-3.
Cursors, indeed, are iterators
that allow simple iteration using \term{for}.

Here is an example of an \term{insert} statement
(we assume that the connection, `c', was already established):

\begin{python}
\begin{lstlisting}
with c.execute("insert into edge (edge, origin, product, timestamp) \
                                 ('complains', 9000001, 100002, \
                                  '1929-01-23T08:45:00')") as rep:
       if not rep.ok():
          print "ERROR: %s" % cur.details()
          exit(1)
       else:
          print "rows affected: %d" % rep.affected()
          print "running time : %d" % rep.runTime()
\end{lstlisting}
\end{python}

In the case of data manipulation (\acronym{dml})
or data loading (\acronym{dll}) statements,
the result may be a status (as with all statements),
namely when the statement fails,
or a report.
A report has the methods \term{affected()},
which indicates the number of rows affected by this statement,
\term{runTime()},
which indicates the running time of the statement in microseconds,
and \term{errors()},
which, in the case of \term{load},
indicates the number of rows that resulted in an error.

For more details on the Python client,
please refer to chapter \ref{chpt_pythonclient}.
The other client \acronym{api}s are described
in chapters \ref{chpt_ccpp}, \ref{chpt_goclient} and
\ref{chpt_luaclient}.

\ignore{
- python in the DB
}
