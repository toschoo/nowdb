\section{Getting Started}

The easiest way to get started
is to use the \nowdb\ docker containing
the database server and clients.

\comment{more instructions of how to get it $\dots$}

The docker does not start the database by itself.
You need to start it explicitly. There is a script
called \tech{nowstart.sh} in the root of the docker
that does that.
(You may want to adapt this script to your specific needs!)

Here is one way to start the docker:

\cmdline{
docker run --rm -p 55505:55505 $\backslash$\\
\hspace*{2cm} -v /opt/dbs:/dbs -v /var/log:/log $\backslash$\\
\hspace*{2cm} -d nowdbdocker /bin/bash -c "/nowstart.sh"
}

This command creates the docker container and starts it.
The parameters are
\begin{itemize}
\item \tech{--rm}
instructs the docker daemon to remove
the container immediately after it will have stopped.

\item \tech{-p 55505:55505} binds the host port \term{55505}
to the same docker port.

\item \tech{-v} maps the host path
\tech{/opt/dbs} to the docker path \tech{/dbs} and
the host path \tech{/var/log} to the docker path \tech{/log}.

\item \tech{-d} means the docker runs in the background
(\term{detached}).

\item \tech{nowdbdocker} is the name of the docker image.

\item \tech{/bin/bash} is the command to be executed
within the docker; \tech{-c} passes a command
to be executed to \tech{bash},
namely \tech{/nowstart.sh}.
\end{itemize}

The script \tech{nowstart.sh}, contains the
instructions to start the \nowdb\ daemon.

Looking into the script,
we see it first sets some environment variables:

\cmdline{
export LD\_LIBRARY\_PATH=/lib:/usr/lib:/usr/local/lib
}

This sets the search path for shared libraries.

\cmdline{
export LUA\_PATH=/lua/?.lua
} 

\cmdline{
export LUA\_CPATH=/lua/?.so
}

\cmdline{
export NOWDB\_LUA\_PATH=$\star$:/lua
} 

These lines sets the search paths for the Lua interpreter
(for Lua modules and C libraries imported by Lua modules)
and for \term{toplevel} modules imported directly by \nowdb\
(we will discuss that later).

Then the script starts the daemon itself:

\cmdline{
nowdbd -b /dbs -l 2>/log/nowdbd.log
}

The script passes two options to the daemon:
the base directory where all databases
managed by this particular daemon live (\tech{-b /dbs})
and the \tech{-l} switch, which activates
server-side Lua support.

Now the daemon is listening to port 55505
and is ready to respond to database requests.
The daemon starts by printing a welcome banner
to standard output:

\begingroup
\small
\begin{minipage}{\textwidth}
\begin{verbatim}
+---------------------------------------------------------------+ 
 
  UTC 2018-10-09T12:03:21.631000000
 
  The server is ready
 
    - with lua support enabled
    - with python support disabled
 
+---------------------------------------------------------------+ 
  nnnn   nnnn          nnnn    nnnnnn       nnnnnn       nnnnnn  
    wi  i   wi       i      i    iw           iw           er   
    wi i     wi     n        n    iw         e  wi        e    
    wii      wi    wi        iw    iw       e    wi      e        
    wi       wi    wi        iw     iw     e      wi    e        
    wi       wi     n        n       iw   e        wi  e        
    wi       wi      i      i         iw e          wie          
   nnnn     nnnn       nnnn             n            n            
+---------------------------------------------------------------+ 

connections: 128
port       : 55505
domain     : any
base path  : /dbs
\end{verbatim}
\end{minipage}
\endgroup

\begin{minipage}{\textwidth}
Here are some options provided by
the \nowdb\ daemon
(for more details on the server interface,
please refer to chapter \ref{chpt_nowdbd}):

\begin{itemize}
\item \tech{-b} The base directory,
where databases are stored.
(default is the current working directory).

\item \tech{-s} the binding domain, default:
any. If set to a host or a domain, the server
will accept only connections from that host
or domain. Example: \tech{-s localhost} does
only accept connections from the server.
The host or domain can be given as name (\term{localhost})
or as \acronym{ip} address (\tech{127.0.0.1});

\item \tech{-p} the port to which
the server will listen; default is 55505,
but any other (free) port may be used.
 
\item \tech{-c} number of connections accepted at the same time.
If the argument is 0, indefinitely many
simultaneous connections are accepted
(until the server is not able to serve more requests);
otherwise, for \tech{-c n},
$n$ being a positive integer,
the server accepts
up to $n$ sessions at the same time and
will refuse to accept more if this number is reached.
The default value for this parameter is 128;

It should be noted that, up to a certain threshold
(which is not configurable) sessions are
reused, that is, sessions enter a connection pool
from which they are fetched, when new connection
requests arrive.

\item \tech{-q} runs in quiet mode
(\ie\ no debug messages are printed to standard error);
\item \tech{-n} does not print the starting banner;
\item \tech{-y} activate server-side Python support;
\item \tech{-l} activates server-side Lua support;
\item \tech{-V} prints version information to standard output;
\item \tech{-?} or \tech{-h} prints a help message to standard error.
\end{itemize}
\end{minipage}

\section{First Steps}
Once \term{nowdbd} is running, we can connect to pass
queries to the server. There is a tool to do this from the command line
called \term{nowclient}. Here is a usage example:

\cmdline{
nowclient -d retail -Q "select count(*) from buys where customer=12345"
}

In this form, the client will try to connect to a server
running on the same host and listening to port 55505.
Furthermore, it will request to use the database \term{retail}
and send the query indicated by the \tech{-Q} parameter.

\begin{minipage}{\textwidth}
If successful, the client will print some
processing information to standard error and the query result
to standard output, \eg:

\cmdline{
executing "use retail" \\
OK \\
executing "select count(*) from buys where customer=12345" \\
59
}
\end{minipage}

\begin{minipage}{\textwidth}
With option \tech{-q} we suppress the processing information.
We would then only see the result:

\cmdline{
59
}
\end{minipage}

\begin{minipage}{\textwidth}
Here are more options supported by the client tool:
\begin{itemize}
\item \tech{-s} 
The server address or name, \eg\ \tech{myserver.mydomain.org} or
\tech{127.0.0.1}. Default is \tech{127.0.0.1};

\item \tech{-p}
the port to which the database is listening. Default: 55505;

\item \tech{-d}
the database to which we want to connect. 
Default: no database at all, which means
that we cannot send queries without naming a database.
Below we will look at alternatives to using this parameter;

\item \tech{-Q}
the query we want the database to process;

\item \tech{-t}
print some (server-side) timing information to standard error;

\item \tech{-q}
quiet mode: don't print processing information to standard error.

\item \tech{-V} prints version information to standard output;
\item \tech{-?} or \tech{-h} prints a help message to standard error.
\end{itemize}
\end{minipage}

The client tool is able to read from standard input;
this way, more than one query can be processed by one call
to \term{nowclient}.
The following command processes the same query as the one above,
but uses standard input instead of the options \tech{-d} and \tech{-Q}:

\cmdline{
echo "use retail;select count(*) from buys where customer=12345;" $| \backslash$ \\
\hspace*{2cm} nowclient
}

Notice that, using standard input,
we need to terminate single \sql\ statements
by a semicolon. This is even true for the last statement.
Leaving the semicolon out would lead to an error.

Of course, we can do much more useful things than just
getting rid of the options.
The main point of reading from standard input is
that we can put \sql\ statements into a file and
$cat$ it to nowclient. A useful example may be:

\begin{sqlcode}
\begin{lstlisting}
drop schema retail if exists;
create schema retail; use retail;

create large storage sales set stress=constant;
create storage statistics;

create type product (
        prod_key uint primary key,
        prod_desc text,
        prod_price float
);
create type client (
        client_key uint primary key,
        client_name text
);

create stamped edge buys (
        origin client,
        destin product,
        quantity uint,
        price  float
) storage=sales;

load '/opt/data/products.csv' into product use header;
load '/opt/data/clients.csv' into client use header;
load '/opt/data/sales.csv' into buys use header;
\end{lstlisting}
\end{sqlcode}

Let's assume we had this code in a file called
\tech{create\_retail.sql}; then we could
send it to \term{nowclient}:

\cmdline{
cat create\_retail.sql | nowclient
}

which would create the retail database.

The script shows some of the peculiarities of \nowdb.
The beginning is quite regular \sql:

\begin{sqlcode}
\begin{lstlisting}
drop schema retail if exists;
create schema retail; use retail;
\end{lstlisting}
\end{sqlcode}

The first line drops the database retail,
\ie\ it removes all its data physically
from disk. The \term{if exists} clause
is included to avoid an error 
(and hence the termination of the script)
in the case
the database does not yet exist.

In the second line the schema `retail'
is created.
The third statement (still in the second line)
instructs \nowdb\ to use the newly created
schema `retail' in all following statements.

The next line is a bit uncommon:

\begin{sqlcode}
\begin{lstlisting}
create large storage sales set stress=constant;
\end{lstlisting}
\end{sqlcode}

The statement creates a \term{storage} object called `sales';
A \term{storage} is a container for 
\term{types} and \term{edges}
(which we will sometimes collectively call \term{tables},
even though this is not the exact same concept
as tables in relational databases).
A storage, hence, is similar to what is called
\term{tablespace} in other databases.

The statement explicitly says that we
want a \emph{large} storage and that there
will be \emph{constant} stress (\ie\ ingestion load)
on the objects in this storage.
It is good policy to put objects with equal
storage parameters,
into the same storage. This is especially true for sizing
(\term{tiny}, \term{small}, \term{big}, \etc).
On the other hand, there are good arguments to
put objects with very high ingestion load into
separate storage containers. This is discussed
with more detail in chapters \ref{chpt_sql} and
\ref{chpt_sizing}.

The next two blocks of code create the types
`product' and `client'.
Types describe vertices.
Each database has a set
of vertices that can be connected
by means of edges to form graphs.
The graphs structure all data
in our specific application.
The vertex types thus describe the universe of discourse.
Technically, vertices are stored in
\term{column-oriented} tables.
\comment{Well, not entirely true at the moment,
but it is the direction to go -- sooner or later
the statement will be true $\dots$}

The attribute types used in the script
are \term{uint}, \term{float} and \term{text}.
The first is a 64bit unsigned integer;
the second is a 64bit floating point number
(a.k.a. \term{double} in languages like C);
\term{text} is a string of up to 255 bytes,
which represent \acronym{utf}-8 characters.
For more information on \sql\ types,
please refer to chapter
\ref{chpt_sql}.

Every vertex type needs a \term{primary key} and
that primary key must consist of only one attribute.
There are no composed keys like in relational
databases.

The types are stored in a default storage
that is created and managed internally by \nowdb.
Every database has two internal storage containers,
one for types and one for edges. If no storage
is indicated in a table (\ie\ type or edge) definition,
the table goes in either of these storage containers,
depending on its type.

The next block defines an \term{stamped} edge.
Edges are links between vertices. Regular edges
have only two fields, namely: \term{origin}
and \term{destination} (which is usually abbreviated
to \term{destin} or even \term{dest}).
A regular edge would be created by a statement of the form:

\begin{sqlcode}
\begin{lstlisting}
create edge from_a_to_b (origin a, destin b) 
\end{lstlisting}
\end{sqlcode}

where $a$ and $b$ are vertex types.

Stamped edges have at least one additional field,
namely \term{timestamp} (which is usually abbreviated
to \term{stamp}). This field does not need not to be mentioned
in the edge definition (in fact, it \emph{must} not be
mentioned). Furthermore, stamped edges may have an
arbitrary number of user-defined fields.
(Well, not really arbitrary: there is a maximum number
of 99 fields).
The edge `buys' defined above has two user-defined
attributes: `quantity', a \term{uint}, and
`price', a \term{float}.

The \term{origin} of buys is of type
\term{client} and \term{destin} is of type \term{product}.
We could, thus, read this definition as a sentence
that says something like: ``client buys product'' or,
since we have also a timestamp which is not explicit
in the definition:
``client buys product at a certain point in time''.
Considering the user-defined fields,
we arrive at a sentence containing even more information:
``client buys quantity of product for price at a certain point in time''.
This is the fundamental idea of \nowdb.

The definition of 'buys' contains a \term{storage} option.
This is the way to tell \nowdb\ that we want this table 
to be stored in a particular storage. The edge 'buys',
hence, goes into the storage 'sales' (which might or
might not contain also other tables).

You may have noticed that we have not created indices.
It is possible to define indices on any field
or combination of fields as in a relational
database using the conventional \term{create index}
statement (please refer to chapter \ref{chpt_sql}
for details). Some indices, however, are created
and managed internally, in particular

\begin{itemize}
\item the primary key of every type is powered by an index
\item every edge (stamped or not) has an index on origin and
\item an index on destination.
\end{itemize}

Thanks to these indices, queries on vertices using
the primary key or on edges involving origin or destination
are fast.
Data access through indices
depends mainly on the size of the result set
and only marginally on the size of the table in question.
One can therefore expect sub-millisecond response time for
queries containing tens of thousands of elements in the result set.
Obviously, processing time grows with the size of the result set.
But it does not grow proportionally
in the size of the table itself.
Querying the data of a specific origin or destination
is not significantly slower with a table containing
hundreds of millions or even billions of rows compared to a small
table with thousands or hundreds of thousands of rows.

The storage engine additionally favours queries
on edges going \emph{forward}, \ie\
searching destinations for a known origin.
Those queries need less disk \acronym{i/o} than queries
with other search criteria.
Searching \emph{backwards} needs more disk \acronym{i/o},
but is still faster than searching with criteria
that do not involve either origin or destination,
\eg\ queries that involve user-defined indices.
The worst, of course, is a \term{fullscan}, especially
on large tables. In such cases, restricting the
result set by time can accelerate the query
significantly.
More details on query
and data optimisation
can be found in chapter \ref{chpt_opt}.

In the final section,
the script loads data from three different \acronym{csv}s
into the database. \nowdb\ provides loaders
for various formats. \acronym{csv} is just one example.
There are many more and the loader even allows 
users to define
their own formats using Apache Avro.
With Avro it is possible to define binary formats, which
can be much faster than textual representations
such as \acronym{csv}.

Using a serialisation system like Avro also eases
interoperability of the database with
external systems and applications and it
significantly eases version management should data formats
change over time (what they always do). 
For more details on data loaders, please refer to chapters
\ref{chpt_sql} and \ref{chpt_loader}.

Loaders, in general, are usually much more efficient than
the \sql\ \term{insert} statement.
The drawback of \term{insert} is that each statement
needs the whole cycle of \sql\ parsing and execution,
while loaders only need one cycle. Since a data source
can contain millions or even billions of rows,
loading is way more efficient than inserting in most cases.

Notice the \term{use header} clause in the loader statements.
It indicates that the \acronym{csv} files actually
have a header line and that we want \nowdb\ to use this header
to identify the columns.
This way, the \acronym{csv} is independent
of the order of columns within the rows.
We could also decide to \term{ignore} the header;
in that case, the first line
in the \acronym{csv} will be ignored;
or we could not mention headers at all.
Then, the first line is interpreted as a regular
line containing data.
In both cases, the fields will be expected to be
in \term{canonical} order, \ie\ the order of fields in
the definition of the type or edge (where
origin, destin and timestamp are always
the first three fields in stamped edges).

\section{First Queries}
The alternative to loading data is, of course, the conventional
\term{insert} statement:

\begin{sqlcode}
\begin{lstlisting}
insert into client values (9000001, 'Popeye the Sailor');
insert into product values (100001, 'Spinach, 450g net', 1.99);
\end{lstlisting}
\end{sqlcode}

These two statements insert a client and a product respectively.
We can also name the attributes explicitly, like:

\begin{sqlcode}
\begin{lstlisting}
insert into product (prod_key, prod_desc, prod_price)
             values (100002, 'Candy Cigarettes, 20', 2.49);
\end{lstlisting}
\end{sqlcode}

Now we insert a bunch of edges:

\begin{sqlcode}
\begin{lstlisting}
insert into buys  (origin, destin, stamp, weight, weight2)
  values (9000001, 100001, '1929-01-17T09:35:12', 1, 1.99)
insert into buys  (origin, destin, stamp, weight, weight2)
  values (9000001, 100001, '1929-01-19T10:15:01', 2, 3.98)
insert into buys  (origin, destin, stamp, weight, weight2)
  values (9000001, 100001, '1929-01-20T17:12:55', 3, 5.97)
insert into buys  (origin, destin, stamp, weight, weight2)
  values (9000001, 100001, '1929-01-22T08:27:32', 1, 1.99)
insert into buys  (origin, destin, stamp, weight, weight2)
  values (9000001, 100001, '1929-01-25T12:09:59', 1, 1.99)
insert into buys  (origin, destin, stamp, weight, weight2)
  values (9000001, 100001, '1929-01-26T21:19:44', 2, 3.98)
insert into buys  (origin, destin, stamp, weight, weight2)
  values (9000001, 100002, '1929-01-22T08:27:51', 1, 2.49)
\end{lstlisting}
\end{sqlcode}

\begin{minipage}{\textwidth}
Worth noticing here is the time format,
which follows \acronym{iso}-8601.

The format is
\begin{itemize}
\item 4 digits for the year and hyphen
\item 2 digits for the month and hyphen
\item 2 digits for the day of month
\item `T' to mark the beginning of the time section
\item 2 digits for the hour and colon
\item 2 digits for the minute and colon
\item 2 digits for the second and,
\item if finer grain is necessary,
a dot followed by up to 9 digits
for the nanoseconds.
\end{itemize}
\end{minipage}

This format can be used anywhere in \nowdb\ \sql.
However, it is also possible to define custom
date and time formats. How to do this is discussed
in chapter \ref{chpt_sql}.

Worth noticing is also the first date.
It was on Jan, 17, 1929 that Popeye had his first
appearance in a newspaper of the \term{King Features}
Syndicate.

Now that we have inserted some data
into our database, we are able to perform $selects$, \eg:

\cmdline{
nowclient -d retail -Q
"select count(*) from buys $\backslash$ \\
\hspace*{4.9cm} where origin=9000001"
}

which would give us 7 and would count Popeye's visits to the supermarket.
We can also count how often Spinach was bought:

\begin{sqlcode}
\begin{lstlisting}
select count(*) from buys
 where destin=100001
\end{lstlisting}
\end{sqlcode}

which shows us 6.
Or we can ask how much Popeye bought and paid per type of product:

\begin{sqlcode}
\begin{lstlisting}
select destin, count(*), sum(quantity), sum(price)
  from buys
 where origin=9000001 
 group by edge, destin
\end{lstlisting}
\end{sqlcode}

\begin{minipage}{\textwidth}
which would give us:
\begin{verbatim}
100001;6;10;19.9000
100002;1;1;2.4900
\end{verbatim}
\end{minipage}

Notice that the output
of the client tool does not resemble the classical
pretty-printed output produced by most database
client tools today. The advantage of such output is
that it is easier for humans to read.
The \acronym{csv}-like output shown above, however,
is better for interoperability, for instance,
when we want to combine it through pipes
with other programs, like this:

\cmdline{
nowclient -d retail -Q "select * from buys" | cut -d";" -f2 | $\dots$
}

On the other hand, there are tools that
produce more readable output from \acronym{csv} input,
such as \tech{csvlook} from the \tech{csvkit} package.\footnote{Have
a look at
\url{https://github.com/jeroenjanssens/data-science-at-the-command-line}}
To obtain a traditional pretty-printed output we could do the following
(assuming that \tech{query.sql} contains the query we used above):

\cmdline{
cat query.sql | nowclient | csvformat -d";" | $\backslash$ \\
    header -a 'product,count,quantity,price' | csvlook
}

and would obtain for the grouping query used above:

\begin{minipage}{\textwidth}
\begin{verbatim}
|----------+---------+----------+---------|
|  product | count   | quantity | price   |
|----------+---------+----------+---------|
|  100001  | 6       | 10       | 19.9000 |
|  100002  | 1       | 1        | 2.4900  |
|----------+---------+----------+---------|
\end{verbatim}
\end{minipage}

The disadvantage of the \tech{csvkit} package is that
it is somewhat slow compared to the native \nowdb\ tools.
For this reason, there is a more complete client tool
for \nowdb\ (which can be downloaded at xxx).
This tool comes with a much larger feature set
and is described in another document.

Here is a more typical time series query illustrating
the advantage of the pretty printer:

\begin{sqlcode}
\begin{lstlisting}
select destin, stamp, quantity, price
  from buys
 where origin=9000001 
 order by stamp
\end{lstlisting}
\end{sqlcode}

\begin{minipage}{\textwidth}
which, with the same technique as above, shows:
\begin{verbatim}
|----------+---------------------+----------+---------|
|  product | stamp               | quantity | price   |
|----------+---------------------+----------+---------|
|  100001  | 1929-01-17T09:35:12 | 1        | 1.9900  |
|  100001  | 1929-01-19T10:15:01 | 2        | 3.9800  |
|  100001  | 1929-01-20T17:12:55 | 3        | 5.9700  |
|  100001  | 1929-01-22T08:27:32 | 1        | 1.9900  |
|  100002  | 1929-01-22T08:27:51 | 1        | 2.4900  |
|  100001  | 1929-01-25T12:09:59 | 1        | 1.9900  |
|  100001  | 1929-01-26T21:19:44 | 2        | 3.9800  |
|----------+---------------------+----------+---------|
\end{verbatim}
\end{minipage}

We can also select from vertices,
but instead of a table, we use the type:

\begin{sqlcode}
\begin{lstlisting}
select prod_price from product
 where prod_key = 100001;
\end{lstlisting}
\end{sqlcode}

which shows
\begin{verbatim}
1.99
\end{verbatim}

and

\begin{sqlcode}
\begin{lstlisting}
select client_name from client
 where client_key = 9000001;
\end{lstlisting}
\end{sqlcode}

which gives
\begin{verbatim}
Popeye the Sailor
\end{verbatim}

Much more typical for \nowdb, however,
is to use vertices together with edges. Edges connect vertices
and can therefore be seen as `relations' in \nowdb.
What we typically want is to combine attributes
of vertices and of edges.
For instance, we may want to look at the attributes
of the destination for a known origin;
or we want to look at edges with the attributes
of both vertices, origin and destin, added to them.

Both patterns are, in \sql, instances of \term{joins}.
An instance of the first pattern would be:

\begin{sqlcode}
\begin{lstlisting}
select stamp, prod_desc, prod_price
  from buys join product on destin
 where origin = 9000001;
\end{lstlisting}
\end{sqlcode}

\begin{minipage}{\textwidth}
\begin{verbatim}
|---------------------+----------------------+--------|
| stamp               | product              | price  |
|---------------------+----------------------+--------|
| 1929-01-17T09:35:12 | Spinach, 450g net    | 1.9900 |
| 1929-01-19T10:15:01 | Spinach, 450g net    | 1.9900 |
| 1929-01-20T17:12:55 | Spinach, 450g net    | 1.9900 |
| 1929-01-22T08:27:32 | Spinach, 450g net    | 1.9900 |
| 1929-01-25T12:09:59 | Spinach, 450g net    | 1.9900 |
| 1929-01-26T21:19:44 | Spinach, 450g net    | 1.9900 |
| 1929-01-22T08:27:51 | Candy Cigarettes, 20 | 2.4900 |
|---------------------+----------------------+--------|
\end{verbatim}
\end{minipage}

Note the difference in the price column.
In the previous query we used \term{price} of buys,
which (as you may have realised)
is the multiplication of product price and quantity.
Here, however, we use the value in \term{prod\_price}
which is the base price of one unit of that product.

We can, of course, combine joins with grouping:

\begin{sqlcode}
\begin{lstlisting}
select destin, count(*), sum(prod_price)
  from buys join product on destin
 where origin = 9000001
 group by destin;
\end{lstlisting}
\end{sqlcode}

\begin{minipage}{\textwidth}
\begin{verbatim}
|---------+-------+---------|
| product | count | price   |
|---------+-------+---------|
| 100001  | 6     | 11.9400 |
| 100002  | 1     | 2.4900  |
|---------+-------+---------|
\end{verbatim}
\end{minipage}

The point about the first joining pattern
is that it joins only one of the two vertices
with the edge. The second pattern is a bit more complex:

\begin{sqlcode}
\begin{lstlisting}
select stamp, prod_desc, client_name
  from buys
  join product on destin
  join client on origin
 where origin = 9000001;
\end{lstlisting}
\end{sqlcode}

The result of this query would be:

\begin{minipage}{\textwidth}
\begin{verbatim}
|---------------------+----------------------+-------------------|
| stamp               | product              | client            |
|---------------------+----------------------+-------------------|
| 1929-01-17T09:35:12 | Spinach, 450g net    | Popeye the Sailor |
| 1929-01-19T10:15:01 | Spinach, 450g net    | Popeye the Sailor |
| 1929-01-20T17:12:55 | Spinach, 450g net    | Popeye the Sailor |
| 1929-01-22T08:27:32 | Spinach, 450g net    | Popeye the Sailor |
| 1929-01-25T12:09:59 | Spinach, 450g net    | Popeye the Sailor |
| 1929-01-26T21:19:44 | Spinach, 450g net    | Popeye the Sailor |
| 1929-01-22T08:27:51 | Candy Cigarettes, 20 | Popeye the Sailor |
|---------------------+----------------------+-------------------|
\end{verbatim}
\end{minipage}

The joining patterns where the attributes of vertices
are directly integrated with edges are so common
and for the graph model so natural that there is
another way to do this which does not look like
relational joins at all, namely \term{paths}.
Indeed, edges form paths through the graph
and we can use them directly to walk from
vertex to vertex without further indication
of how to follow links.

\comment{to be continued}

\ignore{

Syntactically a path is a list of alternating
edges and vertices separated by dots
(starting either from an edge or a vertex),
\eg:

\begin{sqlcode}
\begin{lstlisting}
select buys.price
  from client
 where client_key = 12345
\end{lstlisting}
\end{sqlcode}
}


\section{The Python Client(s)}
Until here we always used the client \emph{tool}
to perform queries.
That is certainly an important use case.
Much more typical, however, is to develop application code
that needs a client \acronym{api} to connect to the database.

\nowdb\ comes with a native client \acronym{api}
that is available in different languages, among others
C, \CC, Go, Python and Lua.

For Python, there are in fact
two \acronym{api}s. One is very close to the underlying
C implementation and, hence, very simple and very fast.
The other complies with the Python DB \acronym{api}
\acronym{pep} 249; it is much slower than the simple
one, but also more convenient and, most importantly,
it can be used with other Python modules such as Pandas.

The module implementing the simple Python \nowdb\ \acronym{api}
is called \term{now.py} and must be imported into the client program.
For the Python interpreter to find this module,
it must be in a directory in the \acronym{pythonpath}.
There may be different ideas on how to install python modules.
The \nowdb\ installation will copy all \nowdb-related modules
to one specific folder and add this folder to the
\acronym{pythonpath}. But you also may install
the \nowdb\ Python \acronym{api} using \tech{pip}.
Then, everything is handled by the Python environment
and you don't need to care about these things.
For more details on installation, please refer
to chapter \ref{chpt_install}.

Anyway, here is a simple Python program:

\begin{python}
\begin{lstlisting}
import now

with now.Connection("localhost", "55505", None, None) as c:
   with c.execute("use retail") as r:
       if not r.ok():
          print "cannot use retail: %s" % r.details()
          exit(1)

   with c.execute("select count(*), sum(quantity), \
                                    avg(quantity)  \
                     from buys") as cur:
       if not cur.ok():
          print "ERROR: %s" % cur.details()
          exit(1)
       for row in cur:
           print "count: %d, sum: %d, avg: %.2f" %
                 (row.field(0), row.field(1), row.field(2))
\end{lstlisting}
\end{python}

The program first creates a \term{Connection}
to a database listening on port 55505 on `localhost'.
It then executes a \term{use} statement on this connection
to indicate the database towards which
the following statements are directed.

The result of a \term{use} statement is a \term{status}.
A status is either \acronym{ok} or an error
whose details can be obtained by means of the method
\term{details()}, which returns a string.
Results also support the method \term{code()},
which would return a numeric error code.
It is often useful to know the error code to decide
what to do programmatically (abandon the program,
retry, try something else, \etc)

Results are \term{resource managers}. That means
that they can be used inside a \term{with} statement.
\term{with} assures that all resources (in this case
the result) are freed before the control leaves
the \term{with}-block even if an exception is raised.

There is no harm in using results without a \term{with} statement.
The Python garbage collector (\acronym{gc}) will invoke the method
that internally frees all C resources related to results.
However, the programmer has little control over when
the \acronym{gc} is invoked. It is therefore good style
to use \term{with} blocks. This guarantees that all resources
are freed as soon as the program does not need them anymore.

The program above then executes a query.
This time the execution returns a \term{cursor} (`cur').
The program checks whether the cursor is
in a good state. The statement may have
failed on the server side. The cursor would
then be in a state that is not \acronym{ok}.
In that case, the program prints the error details
and exits with return code 1.

Otherwise, if the result was fine,
it iterates over the cursor
printing for each row the fields 0-2.
Cursors are iterators
that allow simple iteration using \term{for}.
The \term{for} statement additionally frees
the resources related to \term{row} results
as soon as possible. It, hence, fulfills
a similar function as the \term{with} statement.

There is a convenience interface that
eliminates the explicit error handling
present in the code above.
It is called \term{rexecute}
where the `r' stands for \term{raise}
indicating that errors will be raised
and not returned.
Indeed, whenever the result is not \acronym{ok},
\term{rexecute} raises an error.
This way, only positive results are finally
passed on to the caller.

\begin{minipage}{\textwidth}
Here is an example of an \term{insert} statement
using the \term{rexecute} interface
(we assume that the connection, `c', was already established):

\begin{python}
\begin{lstlisting}
with c.rexecute("insert into buys (origin, product, timestamp) \
                           values (9000001, 100002, \
                                  '1929-01-23T08:45:00')") as rep:
     print "rows affected: %d" % rep.affected()
     print "running time : %d" % rep.runTime()
\end{lstlisting}
\end{python}
\end{minipage}

In the case of data manipulation (\acronym{dml})
or data loading (\acronym{dll}) statements,
the result (if there was no error) is a report
A report has the methods \term{affected()},
which indicates the number of rows affected by this statement,
\term{runTime()},
which indicates the running time of the statement in microseconds,
and \term{errors()},
which, in the case of \term{load},
indicates the number of rows that resulted in an error.

In many cases, however, the programmer is not really interested
in the result. She only wants to know if the statement was
successful. For this very common use case, there is a convenient
interface implemented on top of \term{rexecute} which is called
\term{rexecute\_}. The underscore indicates that the function
does not return a result at all and the `r', as for \term{rexecute},
indicates that errors are raised.
We can therefore simplify the code above to  

\begin{minipage}{\textwidth}
\begin{python}
\begin{lstlisting}
c.rexecute_("insert into buys (origin, product, timestamp) \
           values (9000001, 100002, '1929-01-23T08:45:00')")
\end{lstlisting}
\end{python}
\end{minipage}

For more details on the Python client
and on the \acronym{pep} 249 compliant \acronym{api}
please refer to chapter \ref{chpt_pythonclient}.
The underlying C client \acronym{api} is described in
chapter \ref{chpt_ccpp}.
Client \acronym{api}s for other languages are described
in language-specific documentations.

\section{Lua in the Database}
Like many other databases,
\nowdb\ supports
stored procedures and stored functions,
\ie\ user code that is executed within the database.
The main server-side language
for implementing stored procedures and
stored functions is Lua.

The difference between stored procedures
and stored functions is that stored functions
run inside an \sql\ context. They
can be used in a \term{select} clause,
for instance. They are limited, however,
in what they are allowed to do.
In a \term{select} clause, for instance,
they are not allowed to
execute \acronym{ddl}, \acronym{dll} or \acronym{dml}
statements.
It would indeed be very strange when
a query would change the database.

Stored procedures, on the other hand,
cannot run in \sql\ context. They are
executed explicitly by the \term{exec}
statement. In exchange for this limitation,
they get a lot of power:
stored procedures are allowed to run
any \sql\ code including \acronym{dml}, \acronym{dll}
and even \acronym{ddl}.

Stored procedures are composed of two
elements: there interface (or \term{signature})
and their implementation.
The interface defines how the procedure is
to be called; the implementation defines
what it does.

The interface is created by the \sql\ statement
\term{create procedure}, \eg:

\begin{sqlcode}
\begin{lstlisting}
create procedure sales.revenue(pClient uint, pDay time) language lua 
\end{lstlisting}
\end{sqlcode}

This statement defines the interface of a procedure
called `revenue' that takes two arguments:
\begin{itemize}
\item $pClient$ which is an unsigned integer and
\item $pDay$ which is a timestamp.
\end{itemize}
Furthermore, the procedure is written in Lua and
it lives in the module `sales'.
The statement does not define a return value for the procedure.
But all procedures in \nowdb\ return a \term{result type},
polymorphic type that
can be either a status, a report, a row or a cursor.

The \term{toplevel} module `sales',
which contains the Lua code that implements
the procedure `revenue',
must be located in a directory
\nowdb\ knows about, namely 
in a path in the environment variable
\tech{NOWDB\_LUA\_PATH}.
The entries in this variable have the form:
\term{db}:\term{path},
where \term{db} is the name of a database
and \term{path} a common \acronym{unix} path.
Entries are separated by semicolon, \eg:

\cmdline{
export NOWDB\_LUA\_PATH="retail:/opt/retail/lua;otherdb:/my/path/to/lua"
}

Modules imported into toplevel modules via \term{require}
are regularly handled by the Lua interpreter, \ie\
they must be located in a directory in the regular
Lua path.

Once the function $revenue$ is created
and its code is accessible to the database, it
can be executed by an \term{exec} statement:

\begin{sqlcode}
\begin{lstlisting}
exec revenue(9000001, '1929-01-25')
\end{lstlisting}
\end{sqlcode}

\begin{minipage}{\textwidth}
Let's look into the module \term{sales}
to see how the procedure is implemented:

\begin{python}
\begin{lstlisting}
import nowdb
import datetime

def revenue(pClient, pDay):
  try:

    today = nowdb.now2dt(pDay)
    tom = today + timedelta(days=1)

    stmt = "select sum(weight2) from sales "
    stmt += "where edge = 'buys'"
    stmt += "  and origin =" + str(pClient)
    stmt += "  and timestamp >= " + today.strftime(nowdb.TIMEFORMAT)
    stmt += "  and timestamp <  " + tom.strftime(nowdb.TIMEFORMAT)

    with nowdb.execute(stmt) as c:

      if not c.ok():
        return nowdb.makeError(c.code(), c.details()).toDB()

      r = makeRow()
      for row in c: # there is only one row
         r.add2row(INT, row.field(0))

      return r.toDB()

  except Exception as x:
    return nowdb.makeError(USRERR, str(x)).toDB()

def cleanup():
  pass

\end{lstlisting}
\end{python}
\end{minipage}

First, we import the module \term{nowdb}.
This is the equivalent to the \term{now} module
that we imported on client side.
Note, however, that on server side
it is necessary to import the module in this format,
\ie: \term{import nowdb}, not: \term{from nowdb import $\dots$}
Otherwise, \nowdb\ would not be able to initialise the module.
In that case, the database would issue an error 
and not execute the procedure.

The module is initialised, by the way, when the session
terminates (or when the module is first loaded into a session).
This means
that each session, \ie\ each connection to the database,
has its own view of the module. This allows for
global variables, which will have the same lifetime
as the session itself.

Next step in the module is
the definition of the function \term{revenue} with two parameters.
The database will pass the parameters according to the type
information in the interface: the first parameter
will be passed as unsigned integer and the second
will be passed as timestamp.

The entire code of the procedure is encapsulated
in a \term{try/execpt}-block. This, indeed, is
good practice, since otherwise
an exception would terminate the procedure without
passing a result back to the database and without
detailed error information.

In the \term{except}-part, we create
an error with error code `user error' and
the exception string as detailed information.
Notice that we call the $toDB()$ method
and that we in fact return the result of that
method, not the result itself.
The purpose of the method is to separate
the C part and the Python part of the result.
The C part goes back to the database,
the Python part stays in the Python world
to be collected by the Python garbage collector.
Returning a result without calling its $toDB()$
method would pass Python stuff
to the database which would not know how to handle
that.

The main logic of the function
is in the \term{try}-part.
We start by converting the \nowdb-timestamp
into the Python \term{datetime} object \term{today}
using the \term{now2dt} function
(which, by the way,
is also available on client side).
By adding one day to \term{today} we get
\term{tom}(orrow).

With these values and the \term{pClient}
variable, we construct an \sql\ statement,
which is then passed to the \term{execute}
function. Notice that on server-side,
\term{execute} is not a method of another
object or class like \term{connection}.
Indeed, we already are inside the database.
We don't need a connection to talk to it.

Since the \sql\ statement
is a \term{select}, the result we get
back is a cursor. We check that the cursor
is \acronym{ok} and, if so, we work on
its rows using a \term{for} loop.
In fact, we have only
one row -- since we issued a \term{count}
without a \term{group by} clause.

From the cursor row, we create a new row
using the \term{makeRow} function
and add the first (and only) field 
of the cursor's row to it as integer. This row
is returned to the database (using,
of course, its $toDB()$-method).

Note that we could have returned the cursor itself --
since a cursor is also a result type and, as such,
a valid object to be returned by a stored procedure.
We return the row only for the purpose of illustrating
how rows can be created in Python.
\comment{The whole example is quite trivial to be honest.}

The database will then send the result,
that is the row we created,
back to the caller (the \term{exec} statement),
which, in its turn, can handle this result.
When we call the \term{exec} statement
from \term{nowclient} like this:

\cmdline{
nowclient -d retail -Q "exec revenue(9000001, '1929-01-25')"
}

the result would look like this:
\begin{verbatim}
1.9900
\end{verbatim}

\begin{minipage}{\textwidth}
Of course, we can call \term{exec}
also from a Python client and then handle
the result as row, \eg:

\begin{python}
\begin{lstlisting}
with Connection("127.0.0.1", "55505", None, None) as c:
  with c.execute("exec revenue(9000001, '1929-01-25')") as row:
    if not row.ok():
       print "ERROR: %s" % row.details()
       exit(1)
    print "revenue from client 9000001: %d" % row.field(0)
\end{lstlisting}
\end{python}
\end{minipage}

But we have to come back to the server side once again.
There is still a detail that we have not discussed,
namely the $cleanup()$ function at the end of the module.
Modules may (but do not need to) contain this
function. If it is present in the module, it is automatically
called by the database when the session terminates.
For simple code like the one we used
here for illustration, it is not necessary to have a cleanup
function. 

A cleanup is necessary, whenever result types
(cursors, rows, \etc) are stored in global variables.
It is completely legal to store data in global variables.
When the session terminates, the database will reload
the module, so that the next session will start with
fresh instances of those variables. However, when
global variables are holding result types, the underlying
C part of these variables must be released.
Since the database has no knowledge on what is stored
in global variables, the user needs to provide code
to release this memory.

One interesting use case is \term{functional cursors}.
Functional cursors behave like normal cursors,
but they can use the power of the embedded language
to enrich results
using functionality
not available in \sql.
This is discussed in chapter \ref{chpt_pythonemb}.

\section{What's Next?}
This chapter was only a brief introduction
to some important features and the general flavour 
of \nowdb. The remainder of this manual will
discuss the main features more deeply.

The next chapter discusses the \nowdb\ \sql\ dialect.

Chapters \ref{chpt_nowdbd} and \ref{chpt_clienttool}
present the command line tools \tech{nowdbd}
and \tech{nowclient} respectively.

Chapters \ref{chpt_ccpp} -- \ref{chpt_luaclient}
discuss the available native clients in different
languages and \ref{chpt_llc} presents the low-level
C \acronym{api} mainly used to implement
native clients.

The next two chapters, \ref{chpt_pythonemb} and
\ref{chpt_luaemb}, present the server-side
language bindings at more depth.

Chapter \ref{chpt_install} provides detailed information
on installation of server and client on different
platforms.

The next chapters present more features,
namely the loader (\ref{chpt_loader}) and
server-side techniques such as publish and subscribe and
filters (\ref{chpt_pubsub}).

The following chapters 
\ref{chpt_opt} and \ref{chpt_sizing}
discuss technical insight
for application designers and \acronym{dba}s.

Finally, the appendix \ref{chpt_errors} lists server-side
error codes.


