\section{Outline}
\sql\ is a language to store, manipulate
and query data in a database; traditionally
\sql\ is used with relational databases.
In recent years, however, people have
started to use \sql\ also in other context,
such as \term{graph} and \term{timeseries}
databases and new patterns are evolving
in the language to better address those
data models.

\sql\ consists of statements that,
in their turn, consist of clauses.
A statement is a piece of \sql\ code
that by itself constitutes a meaningful
action in the database. Statements are
distinguished in

\begin{itemize}
\item \acronym{ddl}:
Statements that manipulate entities
in the database that hold or define
data
like tables, types, edges, indices,
functions or procedures.

\item \acronym{dml}:
Statements that manipulate data,
\eg\ \term{insert}, \term{update} and
\term{delete}.

\item \acronym{dll}:
Statements that load large volumes of data into the database
or retrieve large volumes of data from the database.

\item \acronym{dql}:
Statements that read data from the database.

\item Miscellaneous:
Statements that do not fall into any
those categories, in particular
\term{use} and \term{exec}.
\end{itemize}

Clauses are parts of statements;
a \acronym{dql} statement, for instance,
typically has a \term{select} clause and
a \term{from} clause and may have
additional clauses (\term{where},
\term{order by}, \etc).

Some clauses can appear in more than
one type of statement. \term{update}
and \term{delete} statements, typically,
have a \term{where} clause, but no
\term{select} clause.

Clauses can be seen as logical building blocks
of \sql. But they cannot live alone.
It is not possible to execute an isolated \term{where}
clause or an isolated \term{from} clause.
The smallest executable unit is therefore the statement.

Clauses are made of keywords, identifiers, numbers, text,
symbols (such as ``$, \dot = ( ) * + -$'') and
whitespace, \ie\ \acronym{ascii}
10 (line break),
13 (carriage return),
9  (horizontal tab)
and 32 (space).

Keywords and identifiers are mutually
exclusive, that is, if $k$ is a keyword,
$k$ cannot be an identifier at the same time.
\comment{This rule is relaxed in most
\sql\ dialects -- and that is a great relief for users,
because \sql\ has an extraordinary large
number of keywords which sometimes makes the choice
of meaningful identifiers a non-trivial task.
At the time of writing, the \nowdb\ parser
does not relax this rule, but it will do so
in the future.}
Keywords are defined by the \sql\ specification,
identifiers are chosen by the user
and refer to entities in the database,
such as tables, types, indices, \etc\

In this specification,
keywords are typeset in boldface
(\eg\ \keyword{table});
identifiers are typeset in italics
(like `mytable' in 
``\keyword{create table} \identifier{mytable}'').

\sql\ is a textual interface.
All statements that are passed to the database
have a textual form. The results produced
by the database, however, are not. They are
binary data which may or may not
contain textual elements.

In \nowdb\ \sql\ statements are strings
of \acronym{utf}-8 characters.
Keywords, identifiers and numbers, however,
must contain only characters
from the \acronym{ascii} subset.
Identifiers are further restricted:
They must start with an \acronym{ascii} 
Latin alphabetic ($a\dots z$ or $A \dots Z$)
and must contain only
alphanumerics or the underscore ($\_$).

Text, by contrast, may contain any
\acronym{utf}-8 character.
\comment{It is already possible to store
\acronym{utf}-8 in the database.
But comparison and sorting are not yet
\acronym{utf}-compliant. This is an
urgent to-do.}

Keywords and identifiers are case-insensitive.
There is no difference in
\term{SELECT}, \term{select} or \term{Select}
and so on.
Text, by contrast, is case-sensitive;
\term{'hello world'} and \term{hello World}
are not the same thing!

\sql\ is a \term{guest} language
that needs some kind of framework
to support it. One way to provide this
framework is the \nowdb\ client,
which provides two means to execute
\sql\ statements in the database,
\ie\ by means of the \tech{-Q} parameter
and by means of standard input.

Another way is a host language
that provides means to pass \sql\ statements
to the database and means to receive
and interpret the results produced by such statements.
For \nowdb, Python, C and Lua (\comment{Lua not yet})
are available as host languages.

The protocol that defines how data are exchanged
between the database and the host system
is not part of this specification.
Currently, native client and server libraries
exist that implement this protocol
without exposing it to the user.
To support open standards in the future, such as
\acronym{odbc} and \acronym{jdbc},
parts of this protocol must be documented
and published.

\section{Types}
A fundamental part of most languages
it their type system. \nowdb\ \sql\
has a very simple type system,
which is static and save.
This type system is used to design
the database and to word
\sql\ statements.
This fundamental type system is called
\sql\ Static Types.

However, since \sql\ is executed
in a host environment, there is a second
type system to describe the \term{results}
of \sql\ statements.
This other type system is even simpler --
it, in fact, consists of only one type.
This other type system, however, is dynamic.
It is therefore called \sql\ Dynamic Types.

In the following, we first present
the Static System and then the Dynamic System.

\subsection{Static Types}
The static types constitute the \nowdb\ \sql\ type system.
The static types can be used in \sql\ statements.
Each type is equipped with a declaration form and
type constructors that create instances of those types.

The declaration form is used in \acronym{ddl} statements
to define types, edges, procedure and functions.
In \acronym{dml}, \acronym{dll} and \acronym{dql} statements,
instances of the types are used, \ie\
types are not explicitly declared, but used implicitly
by means of their constructors, which are sufficient
to determine the type uniquely.

In the case of numeric types
(integers, unsigned integers and floats),
\nowdb\ silently corrects type mismatches where possible.
An unsigned integer inserted into a field where
a signed integer is expected, is implicitly converted
to an integer; correspondingly a signed integer
is converted to an unsigned integer if possible.
If the unsigned integer is out of range or
the signed integer is negative, the statement
is rejected with a type error.

Likewise, signed or unsigned integers are converted to floats
if necessary (and possible) and a float might be converted
to an integer (or unsigned integer) if it actually
represents an integer.

\begin{minipage}{\textwidth}
\textbf{Integer}\\
Declaration: $int$, $integer$ \\
Values: $-2^{63} \dots 2^{63}-1$ \\
Constructors: $\pm n$, where $n$ is an unsigned integer.\\
Null: $+0$ \\
Examples: $-1, +0, +1$
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Unsigned Integer} \\
Declaration: $uint$, $uinteger$ \\
Values: $0 \dots 2^{64}-1$  \\
Constructors: One digit from the range $0\dots 9$
or one digit from range $1\dots 9$ followed by
a sequence of digits ($0\dots9$). \\
Null: $0$ \\
Examples: $0, 1, 2, 1024$, but not: $01$.
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Float} \\
Declaration: $float$ \\
Represents a \term{binary64} \acronym{ieee}-754 floating point number.
For possible values, please refer to the standard or to the table in
\url{https://en.wikipedia.org/wiki/IEEE\_754#Basic\_and\_interchange\_formats}.\\
Constructors: any integer followed by a dot and a sequence of digits,
              optionally followed by $e$ followed by an integer.
              \comment{The exponential form is not yet available.} \\
Null: $0.0$ \\
Examples: $-1.0, 0.0, 1.0, 3.14159, 1.797693e308$ 
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Time} \\
Declaration: $time$, $date$ \\
Values:  \acronym{utc} 1677-09-21T00:12:44 --
         \acronym{utc} 2262-04-11T23:47:16 \\
Precision: nanosecond. \\
Note, however, that range and precision depend on server configuration.
With less precision, a higher range can be reached.
Please refer to the database configuration guide. \\
Timezone: \acronym{utc} \\
Constructor: any integer or any string following \acronym{iso}-8601 
or any string following a locally defined time format. \\
Null: $0$ \\
\comment{It should be possible to perform arithmetic
with time units and timestamps
in \sql, \eg: \\
\term{where timestamp = today + 7*day}.\\
That is, why a null type makes sense for time.
Currently, arithmetic with time is not possible in \sql.
(It is possible with the date and time types in host languages, though,
and therefore not urgent.)} \\
Examples:\\
1535284617906179393, \\
'1940-12-21', \\
'1904-06-16T11:43:10', \\
'2011-11-11T11:11:11.123456789'
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Bool} \\
Declaration: $bool$ \\
Values: $true$, $false$ \\
Null: $false$
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Text} \\
Declaration: $text$ \\
Values: \acronym{utf}-8 string with up to 255 bytes\\
Constructor: string enclosed by ' \\
Null: '' (the empty string),\\
Examples:\\
'hello world',\\
\begin{CJK}{UTF8}{gbsn}
'鎮州臨濟慧照禪師語錄序。' 
\end{CJK} \\
\comment{An important detail is not yet handled:
text that \emph{contains} the character '.
This is important for recursive \sql, \eg\ \\
\term{exec metaquery('select * from myedge 
       where a = $\backslash$'some text$\backslash$'')}}
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Longtext} \\
Declaration: text \\
Values: \acronym{utf}-8 string with up to 4096 bytes\\
\comment{Longtext is not yet available.}
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Blob} \\
\comment{Blob is nice to have, but there are currently
no concrete plans to add such a datatype.}
\end{minipage}

\subsection{Dynamic Types}
Dynamic types are not used in \sql\ statements,
but rather describe the return values of \sql\
statements. As such, they live in the context
of a host language (C, Python, Lua, \etc).
Their concrete implementation, therefore,
depends on that language and any language
functioning as either client or server-side host
needs to implement these types.
For more information on concrete implementation
of dynamic types, please refer to the
host language \acronym{api} specifications.

\begin{minipage}{\textwidth}
\textbf{Status}\\
Values: \acronym{ok}, \acronym{nok}\\
In the case of error (\acronym{nok}), \nowdb\ provides:
\begin{itemize}
\item an error code
\item a detailed error message
\end{itemize}
Error codes together with a brief description
of their meaning can be found in \ref{chpt_errors}.
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Report}\\
Reports consist of up to three values:
\begin{itemize}
\item number of affected rows
(returned by all \acronym{dml} and \acronym{dll} statements)
\item number of errors
(returned only by \acronym{dll} statements)
\item running time
(returned by most \acronym{dml} and \acronym{dll} statements)
\end{itemize}
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Row}\\
A row is the result of a projection;
it consists of an array of values
with type information called \term{fields}.
In the host language, one would access a field
typically by an expression of the form:
$row.field(i);$
which would return a tuple $(value,type)$.

A row result consists of one or many rows.
The host language shall provide means
to iterate over rows.
\end{minipage}

\begin{minipage}{\textwidth}
\textbf{Cursor}\\
A cursor is an iterable collection of rows.
The iteration directive is \term{fetch}.
Each fetch may return one or many rows.
A cursor is a server-side resource
and shall be closed using the directive \term{close}.
\end{minipage}

\section{Data Definition}
\comment{Due to some time pressure in producing this document,
there are no syntax diagrams, which would make things much
clearer. Instead, examples are given. Examples are easier
to understand quickly, but they are much less precise.}

\comment{For almost all \acronym{ddl} subtypes,
\term{alter} is not yet implemented.}

\subsection{Schema}
The keywords
\term{schema}, \term{database} and \term{scope}
are interchangeable.

\subsubsection{CREATE}
The \term{create schema} statement
creates an empty database physically on disk.
It has the following form:

\keyword{create schema} \identifier{mydb}

This would create all objects necessary
to manage that database.

The following forms are equivalent:

\keyword{create database} \identifier{mydb}\\
\keyword{create scope} \identifier{mydb}

All \keyword{create} clauses can be combined
with the clause \keyword{if not exists}, \eg:

\keyword{create schema} \identifier{mydb} \keyword{if not exists}

The \keyword{if not exists}-clause
suppresses the `duplicate key' error
in case the schema
already exists.
It is a convenient way to avoid
that a \sql\ script is abandoned
in such a situation.

\subsubsection{DROP}
The \term{drop schema} statement
removes a database physically from disk.
It has one of the following forms,
which are all equivalent:

\keyword{drop schema} \identifier{mydb}\\
\keyword{drop database} \identifier{mydb}\\
\keyword{drop scope} \identifier{mydb}

The statement removes all objects and data
belonging to the database `mydb' from disk.

All \keyword{drop} clauses can be combined
with the clause \keyword{if exists}, \eg:

\keyword{drop schema} \identifier{mydb} \keyword{if exists}

The \keyword{if exists}-clause
suppresses the `key not found' error
in case the schema
does not exist.
It is a convenient way to avoid
that a \sql\ script is abandoned
in such a situation.

\subsubsection{ALTER}

\subsection{Table}
\subsubsection{CREATE}
The \term{create table} statement
creates a new storage entity for edges
physically on disk.

The simplest form is:

\keyword{create table} \identifier{mytable}

The keyword table may be decorated
with a sizing option:

\keyword{create big table} \identifier{mytable}

Valid sizing keywords are:
\keyword{tiny, small, medium, big, large, huge}.

Sizing keywords affect the allocation units
of disk space. The concrete meaning is not
part of this specification and may change
in the future.

It is also possible to add options to
to a \term{create table} statement.
Options have the general form

\term{\keyword{set} option = value, option = value}.

Valid options and values are listed in the following table:

\bgroup
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{||c||c||c||c||}\hline
Option & Values & Meaning & Default \\\hline\hline
\keyword{stress} & \keyword{moderate} & Low ingestion volume with occasional peaks & X \\\cline{2-4}
                 & \keyword{constant} & Constant ingestion of high volume          &   \\\cline{2-4}
                 & \keyword{insane} & Constant ingestion of very high volume       &   \\\hline\hline
\keyword{disk} & \keyword{hdd}  & Disk space is allocated in large chunks          & X \\\cline{2-4}
               & \keyword{ssd}  & Disk space is allocated in small chunks          &   \\\cline{2-4}
               & \keyword{raid} & Currently, no effect                             &   \\\hline\hline
\keyword{compression} & 'zstd'  & zstd is used for compression                     & X \\\cline{2-4}
                      & 'lz4'   & lz4 is used for compression (\comment{not available}) &   \\\cline{2-4}
                      & ''      & Data in this table are not compressed at all     &   \\\cline{1-4}
\end{tabular}
\end{center}
\egroup

The option \keyword{stress} affects the number of threads
allocated to perform ingestion tasks
like compression, sorting and indexing.
How many threads are allocated
is not part of this specification
and may vary between platforms.

The user may decide on the compression algorithm to use.
The standard compression algorithm is \term{zstd},
which is fast, but has also very good compression ratio.
It is recommended to use \term{zstd} in most cases.
\term{lz4} (\comment{not yet available}) is faster than \term{zstd},
in particular on decompression,
but has a weaker compression ratio.
Finally, no compression at all (empty string)
makes sense on small tables
that are known never to grow beyond some megabyte in size
(or beyond some million edges).

An example of a \term{create table} statement with options is

\keyword{create table} \identifier{mytable}
\keyword{set} \keyword{stress} = \keyword{constant},
              \keyword{compression} = 'zstd'

\subsubsection{DROP}
The \term{drop table} statement removes
an existing storage entity for edges
physically from disk.
It has the form:

\keyword{drop table} \identifier{mytable}

\subsubsection{ALTER}

\subsection{Type}
\term{Types} are user-defined types
stored as vertices in the database.
The syntax resembles very much
the \term{create table} syntax
in traditional \sql:

\keyword{create type} \identifier{product} \{ \\
\hspace*{1cm}\identifier{prod\_key} \keyword{uint} \keyword{primary key}, \\
\hspace*{1cm}\identifier{prod\_desc} \keyword{text}, \\
\hspace*{1cm}\identifier{prod\_price} \keyword{float} \\
\}

This creates a type called \term{product}
with three attributes:
\term{prod\_key}, \term{prod\_desc} and \term{prod\_price}.

There is no limit on the number of attributes
a type may have. Indeed, vertices with hundreds
of attributes are not uncommon.

Attributes may have any static type with one exception:
Each type needs a unique primary key and the field
that is primary key must be either
\keyword{uint} or \keyword{text}.

The order in which attributes are declared
determines the \term{canonical} order for this type.
The canonical order plays a role for the \term{insert}
statement.

\comment{
I am not sure about the choice of the term 
``table''; I am afraid it can lead to confusion
with the relational view.
In fact, types look much more like tables --
and perhaps they should be called like that.
The original name for what is now called
``tables'' was ``context''. For obvious reasons,
I wanted to get rid of that word.
However, I am again finding some sympathy for it.
}

\subsubsection{DROP}
The \term{drop type} statement removes a type
from the database. \comment{
At the moment, drop type removes only the type definition,
not the corresponding data;
in the future, that will certainly change.
}
It has the form:

\keyword{drop type} \identifier{product}

\subsection{Edge}
\subsubsection{CREATE}
The \term{create edge} statement defines the layout
of a specific edge type in the database.
Example:

\keyword{create edge} \identifier{buys} \{ \\
\hspace*{1cm} \keyword{origin} \identifier{client} \keyword{as} \identifier{client}, \\
\hspace*{1cm} \keyword{destin} \identifier{product} \keyword{as} \identifier{product}, \\
\hspace*{1cm} \keyword{label} \keyword{text}, \\
\hspace*{1cm} \keyword{weight} \keyword{int} \keyword{as} \identifier{quantity}, \\
\hspace*{1cm} \keyword{weight2} \keyword{float} \keyword{as} \identifier{paid} \\
\}

Edges have seven fields (in canonical order):
\keyword{edge},
\keyword{origin}, \keyword{destin},
\keyword{label}, \keyword{timestamp},
\keyword{weight} and \keyword{weight2}.

The type of the field `edge' is predefined.
It is always a string and corresponds to the
name of that edge (\eg\ `buys').
The reason that it exists at all
is that edges of different types can be stored
in the same table.
This makes especially sense for groups of edges
with few instances or for groups of edges
that are often read together.
Think for instance of \term{follow} and \term{unfollow}
in a social media application.
We might want to read \term{follow} and \term{unfollow}
events together ordered by timestamp to determine
what the current state is.

Likewise is the type of the timestamp predefined
and cannot be changed.

All other types can be defined.
The types of \keyword{origin} and \keyword{destin}
are usually \keyword{type}s, since they refer
to vertices which are connected by this specific edge.
Stored in the database is the value of the primary key
of the specific type (\ie\ \keyword{uint} for product).

But they also may have static types, namely
\keyword{uint} or \keyword{text}.

The field \keyword{label} as well may be of type
\keyword{uint} or \keyword{text}.

The fields \keyword{weight} and \keyword{weight2}
may have any static type.

Fields that are not mentioned in the edge definition
are invisible and cannot be used.

\comment{
Some aspects of the design are centred around the idea
that it should be possible to insert \textit{ad-hoc}
data, in particular edges,
that were not previously defined in terms of \acronym{ddl}.
More recent design decisions (like the one above)
foreclose this possibility.
This is still an ongoing design process and
some details may change...
}

It is also possible to rename edge fields
according to their real purpose in a concrete
application (\eg\ naming \keyword{weight} \identifier{quantity}).
All fields that may appear in an edge definition
may be renamed in this way.
\comment{Not yet available!}

\subsubsection{DROP}
The \term{drop edge} statement removes an edge definition,
but not the data described by this definition,
from the database.
It has the form:

\keyword{drop edge} \identifier{buys}

\subsection{Index}
\subsubsection{CREATE}
The \term{create index} statement
creates an index physically on disk.
It has the form:

\keyword{create index} \identifier{myidx} \keyword{on} \identifier{mytable}
(\identifier{field1}, \identifier{field2})

\comment{
It is possible to create indices on vertex.
But that is a mess and is not documented here.
In fact, currently, indices on vertex are
not on the user-defined attributes, but on
internal attributes
(like \term{roleid}, \term{property}, \etc).
That, obviously, does not make any sense.
Instead there will be internal indices
that are created, when the database is created,
and user-defined indices, which will then be
defined on the type attributes.
}

The fields (``field1'', ``field2'', \etc)
are edge fields. Any combination of fields
can be used in index definitions.
It is not recommended, however,
to create indices on fields that are defined as \keyword{float}
or \keyword{time} (\eg\ \keyword{timestamp}).

\comment{
There will be the possibility to define indices over ranges
of \keyword{float} fields and periods
of \keyword{time} fields. But that is not yet available.
}

The \keyword{index} keyword can be decorated with a sizing
indication, \eg:

\keyword{create tiny index} \identifier{myidx} \keyword{on} \identifier{mytable}
(\identifier{field1}, \identifier{field2})

The default sizing is \keyword{small}.

It rarely makes sense to create 
\keyword{big}, 
\keyword{large} or 
\keyword{huge} indices.
It actually makes sense,
when the index from the beginning
has many keys with many data points.
More details on this can be found in \ref{chpt_sizing}.

\subsubsection{DROP}
The \term{drop index} statement removes an index physically from disk.
Example:

\keyword{drop index} \identifier{myidx}

\subsection{Procedure}
\subsubsection{CREATE}
The \term{create procedure} statement
creates procedure interface in the database.
It has the form:

\keyword{create procedure} \identifier{mymodule}.\identifier{myfun}(
                           \identifier{param1} \keyword{uint},
                           \identifier{param2} \keyword{text})
                           \keyword{language} \identifier{python}

A procedure may have no parameters.
The definition then simplifies to

\keyword{create procedure} \identifier{mymodule}.\identifier{myfun}()
                           \keyword{language} \identifier{python}

Any number of parameters is allowed and parameters may have
any static type.

Known languages are \identifier{python} and \identifier{lua}.
\comment{Lua is not yet available.}

\subsubsection{DROP}
The \term{drop procedure} statement
drops a procedure interface from the database.
It has the form:

\keyword{drop procedure} \identifier{myfun}

\subsection{Function}
\comment{Not yet available}

\subsection{Period}
\term{Period} is not a \term{first-class citizens}
like types, edges, procedures, \etc\
In particular, periods cannot be created or altered.
They evolve as an effect of inserting edges into
the database. However, periods can be identified
and they can be dropped.

Dropping data according to timestamps is an important
feature in timeseries databases.
Without this feature, databases would grow
to an extent that would make efficient queries difficult
or even impossible. Deleting data by means of the
\term{delete} statement is not efficient for large
amounts of data. In the case of \nowdb\
(but also with other databases), deleting
the data would not solve the problem,
because \term{delete} does not physically remove
the data, but just makes them invisible.

Dropping, by contrast, removes all data files 
that contain only data belonging to that period.
Removing files, however, is a very efficient
operation on most platforms, since it only
declares the disk space free.

The syntax is:

\keyword{drop period on} \identifier{mytable} \\
\keyword{where timestamp} $\ge$ '2018-01-01' \\
\hspace*{0.45cm}\keyword{and timestamp} $<$  '2018-04-01'

This would drop all data files of table `mytable'
that contain data between Jan, 1, 2018 (inclusive) and
April, 1, 2018 (exclusive). 

Two remarks are in place. First,
\term{drop period} must have a \term{where} clause.
There must be at most two conditions in the \term{where} clause
and, if there are two, they must be connected by \keyword{and}.
All conditions must refer to \keyword{timestamp} and there
must be precisely one condition that
determines an \emph{end date}
for the period, \ie\ there must be a non-negated expression
that contains \keyword{timestamp} and a constant date
and one of the operations $<$ or $\le$.

The rationale for this restrictions is
to avoid accidentally dropping entire tables
using too complex or incomplete \term{drop} statements.
The keyword \keyword{between} is not used with \term{drop period}
because \keyword{between} makes constructions
without a start date somewhat artificial.
Furthermore, \keyword{between} does not allow
distinguishing between \emph{in}clusive and
\emph{ex}clusive selections of period limits.

To illustrate that, the following example 
is legal and it drops all data
before a given date (which is very common
for timeseries databases, that often represent gliding
time windows):

\keyword{drop period on} \identifier{mytable} \\
\keyword{where timestamp} $<$ '2018-04-01'

Second, \term{drop period} does not guarantee
to drop all data that lie in the period in question --
in fact, it does not even guarantee to drop any data at all.
It guarantees, however, to remove all files
that \emph{only} data that lie that period.
In other words, the behaviour is conservative
and prefers dropping less data than possible over
dropping too many data.
On the long run, however, with a consistent dropping policy
old data will by removed consistently and the database
won't grow (except when the periods themselves grow).

\section{Data Manipulation}
\subsection{Insert}
The \term{insert} statements inserts one or more rows
to a given table or type.
The basic form is

\keyword{insert into} \identifier{mytable} 
                      (\identifier{myfield1},
                       \identifier{myfield2})
                      (value1, value2)

A more concrete example is

\keyword{insert into} \identifier{product} 
                      (\identifier{prod\_key},
                       \identifier{prod\_desc},
                       \identifier{prod\_price})
                      (100001, 'Spinach', 1.99)

For an edge, this would be:

\begin{minipage}{\textwidth}
\keyword{insert into} \identifier{sales} 
                      (\keyword{edge},
                       \keyword{origin},
                       \keyword{destin},
                       \keyword{timestamp},
                       \keyword{weight},
                       \keyword{weight2})
                      (\\
                       'buys', 9000001, 100001,
                       '1929-01-22T08:53:22',
                       3, 5.97)
\end{minipage}

When the list of values insert is complete,
\ie\ covers all fields in the type or table,
and respects the canonical order,
a shortened form can be used, \eg:

\keyword{insert into} \identifier{product} 
                      (100001, 'Spinach', 1.99)

Note that for the shorthand form on edges,
all typeable edge fields must be declared
in the edge definition. 

It is also possible \comment{(not yet!)} to insert data from
a query, \eg:

\begin{minipage}{\textwidth}
\keyword{insert into} \identifier{sales} ( \\
                      \keyword{edge},
                      \keyword{origin},
                      \keyword{destin},
                      \keyword{timestamp},
                      \keyword{weight},
                      \keyword{weight2}) (\\
\hspace*{0.2cm}\keyword{select} 'buys', \keyword{origin}, 
                         \keyword{destin},
                         \keyword{timestamp},
                         \keyword{weight},
                         \keyword{weight2} \\
\hspace*{0.35cm}\keyword{from} \identifier{another\_table} \\
\hspace*{0.2cm}\keyword{where} $\dots$)
\end{minipage}

For details on the \term{where} clause,
please refer to the \acronym{dql} section.

\subsection{Update}
The \term{update} statement changes the values
of fields in rows in tables or types.
Its general form is

\keyword{update} \identifier{mytable} \\
\hspace*{0.7cm} \keyword{set} field = value,\\
\hspace*{0.7cm} \keyword{set} field = value \\
\hspace*{0.1cm} \keyword{where} $\dots$

For instance:

\keyword{update} \identifier{product} \\
\hspace*{0.7cm} \keyword{set} \identifier{prod\_price} = 1.89 \\
\hspace*{0.1cm}  \keyword{where} \identifier{prod\_key} = 100001

For details on the \term{where} clause,
please refer to the \acronym{dql} section.

\comment{update has still many unsolved issues.
For instance, when changing primary keys
and other indexed fields, we need to delete
that particular row from the index and add it
again with the new value.
That, however, is quite expensive.
It therefore may take still some time
to make update available :-(}

\subsection{Delete}
The \term{delete} statement eliminates single
data points from tables or types.
Its general form is

\keyword{delete from} \identifier{mytable} \keyword{where} $\dots$

A more concrete example:

\keyword{delete from} \identifier{product}
\keyword{where} \identifier{prod\_key = 100001}

For details on the \term{where} clause,
please refer to the \acronym{dql} section.

It is worth mentioning that delete
does not physically remove data from disk.
It marks the corresponding rows as deleted,
so they won't be selected by other statements.

\comment{delete still has bugs and is therefore not yet available :-(}

\section{Data Loading}
\subsection{Create}
The \term{create loader} statement creates a user-defined loader.

\comment{Not yet avalailable}

\subsection{Drop}
The \term{drop loader} statement removes a user-defined loader
form the database.

\comment{Not yet avalailable}

\subsection{Load}
The \term{load} statement loads data from an external
data source into the database.
Its general form is

\keyword{load} '/path/to/datafile' \keyword{into} \identifier{mytype\_or\_edge}

\comment{
For types it's currently ``load '/path/to/datafile' into vertex use header as type''
(where \term{type} here means the concrete type, \eg\ \term{client}).
For edges it's ``load '/path/to/datafile' into mytable as edge''
(where \term{edge} is literal).
The simpler form above is currently reserved for a raw import scheme,
which will soon disappear.
}

The \term{load} statement has optional \term{use} and \term{ignore} clauses
\comment{we also need a clause options to a user-defined loader}:

\keyword{use loader} \identifier{myformat}

With this clause, a user-defined loader is applied.
Without this clause the default loader (\acronym{csv})
is applied. That is equivalent to

\keyword{use loader} \identifier{csv}

In the case of a \acronym{csv} loader,
the \keyword{header} option can be used:

\keyword{use header}

or:

\keyword{use csv, header}

which means that the data source has a header and that the loader
will use this header to determine how to load the columns in the data source.
Note that vertices always require a header, while edges
must not use a header. If the data file actually has a header
and edges are to be loaded, the \term{ignore} clause must be used:

\keyword{ignore header}

This will ignore the first line of the \acronym{csv}.

Here is a complete example for the default loader and vertices:

\keyword{load} '/opt/import/client.csv'
\keyword{into} \identifier{client} \keyword{use header}

\comment{
\keyword{load} '/opt/import/client.csv'
\keyword{into} \keyword{vertex} \keyword{use header as} \identifier{client}
}

and for edges:

\keyword{load} '/opt/import/transactions.csv'
\keyword{into} \identifier{sales}

\comment{
\keyword{load} '/opt/import/transactions.csv'
\keyword{into} \identifier{sales} \keyword{as edge}
}

The \term{load} statement returns a report on success
that indicates how many rows have been loaded,
how many rows have failed and how long it took.
Notice that \term{load} does not stop on errors.
Instead, errors are written to standard error
with the row number where the error occurred.
This way, the faulty lines can be corrected
in reimported later. \comment{The process must still be adapted
to client/server. The server should write the errors
into a specific file; otherwise, when there is more
than one session importing data, errors would refer
to line numbers of different files.}

\subsection{Dump}
\comment{Not yet available}

\section{Data Querying}
There is only one type of \acronym{dql} statement.
However, this statement is much more complex
than the statements we have seen so far.

A \acronym{dql} statement consists of at least
a \term{select} clause (also called \term{projection} clause
and a \term{from} clause, which in itself my contain
a \term{join} clause.
It may additionally contain
a \term{where} clause,
a \term{group} clause and
an \term{order} clause.

\subsection{Select Clause}
The basic form of a \term{select} clause is

\keyword{select} \identifier{projectable1}, \identifier{projectable2}, $\dots$

Where the \term{projectables} may be fields of the entity (edge or type)
referenced in the \term{from} clause.

If all fields of that entity are selected,
the statement can be simplified to

\keyword{select} \keyword{*}

It is also possible to refer explicitly to the data source
(which is identified in the \term{from} clause), \eg:

\keyword{select} \identifier{product}.\identifier{prod\_price}

In the \term{from} clause, we can define aliases for data source
and then refer to the data source by this alias, \eg:

\keyword{select} \identifier{p}.\identifier{prod\_price}

where $p$ is an alias defined in the \term{from} clause.
This is especially useful with joins.

Projectables are not necessary fields.
They also may be functions, constants or even complex expressions
composed of fields, constants, functions and operators.
The following are, for instance, valid \term{select} clauses:

\begin{minipage}{\textwidth}
\keyword{select} \identifier{true} \\
\keyword{select} 'X' \\
\keyword{select} 3.14159 \\
\keyword{select} $sum(weight)/42$ \\
\keyword{select} \keyword{count}(*), \keyword{sum}(\keyword{weight})
\end{minipage}

The first three clauses may appear pointless.
Why select constant values from the database?
There are, however, very common use cases
for selecting constant values, in particular
in combination with the \term{exist} operator,
but also for \acronym{dql} used within an
\term{insert} statement. A concrete example
of the latter already appeared in the section
on \term{insert}, namely:

\keyword{select} 'buys', \keyword{origin},
                         \keyword{destin}, 
                         \keyword{timestamp}, 
                         \keyword{weight},
                         \keyword{weight2}

where a constant expression, namely 'buys',
is combined with a number of fields.

\nowdb\ provides \term{functions} that can be used
on fields \comment{or, in the future, on more than
one field at once; see the comment below!},
for instance:

\keyword{select} substr('hello world', 1, 5)

\comment{None of such functions is implemented yet.}

A special form of functions are \term{aggregates}.
Aggregates do not operate on one row, but
on \term{groups} of rows. A group may be defined
by adding a \term{group} clause. If not \term{group} clause
is present, the group consists of all rows produced
by the \acronym{dql} statement.

The following are valid \term{select} clauses:

\keyword{select} \keyword{count}(*) \\
\keyword{select} \keyword{sum}(\keyword{weight}) \\
\keyword{select} \keyword{avg}(\keyword{weight}) \\
\keyword{select} \keyword{median}(\keyword{weight}) \\
\keyword{select} \keyword{stddev}(\keyword{weight})

The freedom of the \term{select} clause
is restricted by other clauses, in particular
the \term{from} clause and the \term{group} clause.
In the \term{select} clause, only
those fields may appear that are actually
part of the entity chosen in the \term{from} clause.
The interdependencies with the \term{group} clause
are more subtle and will be discussed there.

\comment{
Concerning expressions:
Most \sql\ dialect support a wide range of
arithmetic expressions, \eg\ $+, -, \times, \div, \sqrt{a} $,
boolean expressions, \keyword{and}, \keyword{or}, \keyword{not}, \etc\
string operations like concatenation, substring, length and so on.
Since expressions are recursive, they can be composed, \eg:
$(a+b)/n, (weight-w)^2$ \etc
On the long run, we need to address expressions.
The point of the prototype, however, is not
to prove that \sql\ is possible,
but that designing special-purpose database technology
for specific data models
can improve performance and usability.
Full-fledged \sql\ support will therefore be added,
when the main argument is made.
}

\subsection{From Clause}
The \term{from} clause determines the data source
of the \acronym{dql} statement. The simplest form
of a \term{from} clause is

\keyword{from} \identifier{mytable}

The identifier must refer to a table or a type.
Valid \term{from} clauses are for instance:

\keyword{from} \identifier{sales} \\
\keyword{from} \identifier{product} \\
\keyword{from} \identifier{client}

\comment{
The latter two, unfortunately, do not work right now.
The current syntax is quite esoteric (and will go
away very soon). Since \term{product} and \term{client}
are vertex types, the current form is
\keyword{from vertex as} \identifier{product}.
That is quite ugly and, indeed, pointless.
}

Here is a first example of a complete \acronym{dql} statement:

\keyword{select} 'buys', \keyword{origin},
                         \keyword{destin}, 
                         \keyword{timestamp}, 
                         \keyword{weight},
                         \keyword{weight2}
\keyword{from} \identifier{sales}

It is possible to define an alias for the data source:

\keyword{from} \identifier{sales} \keyword{as} \identifier{s}

The table \identifier{sales} can now be referred to as ``s''
in all other clauses.
This technique is especially interesting in combination
with joins.

In most \sql\ dialects,
it is common to select data from different
data sources at once, \eg:

\keyword{from} \identifier{product}, \identifier{client}

This form is not supported by \nowdb.
Whenever, more than one data source is addressed,
a join has to be used.

\subsection{Join Clause}
A join combines an edge with a vertex.
\comment{
Edges are in fact the way to express
relations between vertices.
There is no other way to combine vertices.
Perhaps, in the future, we need something else.
Many graph databases offer additional links
between vertices, which are not edges.
Usually, these links are used for connections
that are part of the characteristic of the
vertex. One could say that such links express
``o que \'e'', while edges express
``o que est\'a''.
}
The basic form is:

\keyword{join} \identifier{mytype} \keyword{on} \keyword{edgefield}

To make this more concrete:

\keyword{from} \identifier{sales} 
\keyword{join} \identifier{client} \keyword{on} \keyword{origin}

This would produce an \term{inner join} 
between \identifier{sales} and \identifier{client}.
Joins in \nowdb\ are in fact always inner joins.
There are no \term{outer joins}. \comment{Do we need them?}
In consequence, there is not difference between
\term{left} and \term{right}; one could say,
joins in \nowdb\ are \term{abelian}.

Since the primary key of a type is known
and there is always exactly one field
which is primary key, the foreign join key
(that of \identifier{client}) needs no
explicit mentioning. It is implicitly clear 
that \keyword{join} \identifier{client} \keyword{on origin}
joins on \keyword{origin} $=$ \identifier{client\_key}.

Each edge connects two vertices.
A join, therefore, consists of at most two joins.
The syntax is straightforward:

\keyword{from} \identifier{sales}
\keyword{join} \identifier{client} \keyword{on} \keyword{origin} \\
\hspace*{2cm}\keyword{join} \identifier{product} \keyword{on} \keyword{destin}

With this join, all attributes
from \identifier{sales}, \identifier{client} and \identifier{product}
are available in the \term{select} clause.

It may happen that the joined entities
have fields with the same name.
We could have named the primary key in both,
\identifier{client} and \identifier{product},
\identifier{key}, instead of
\identifier{client\_key} and \identifier{prod\_key}.
To distinguish the fields,
one has to use the entity name together with the field name
in the \term{select} clause and (as we will see) in all
other clauses that refer to fields.
Example:

\keyword{select} \identifier{client}.\identifier{key}

Here, using aliases comes in handy, \eg:

\keyword{from} \identifier{sales} \keyword{as} \identifier{s}
\keyword{join} \identifier{client} \keyword{as} \identifier{c} 
               \keyword{on} \keyword{origin} \\
\hspace*{2.8cm}\keyword{join} \identifier{product} \keyword{as} \identifier{p}
                              \keyword{on} \keyword{destin}

In the \term{select} clause, we can now refer to fields
with the alias instead of the full entity name, \eg:

\keyword{select} \identifier{c}.\identifier{key},
                 \identifier{p}.\identifier{key}

\comment{
Joins are the very heart of any database technology --
but it is not easy to get the them right with good performance.
In a first approach I tried to make joins work
by always using index range scans.
Unfortunately, it turned out that index range scans
are not very efficient in most (or at least many) cases.
I am therefore looking for alternatives.
There are many, sometimes quite sophisticated, techniques.
Currently, I am experimenting with cache-based subqueries,
which looks promising. Joins can therefore be available
within about two weeks. (And this is indeed one of the most
urgent items on the to-do list.)
}

\subsection{Correlation} 
Correlation combines two or more edges.
\comment{I believe that correlation is much better placed
in support libraries of high-level languages than in \sql.}

\subsection{Where Clause}
The \term{where} clause adds criteria for the selection
of specific rows. \term{where} clauses can be very complex.
Indeed, a \term{where} clause is one complex \term{Boolean} expression.
\comment{Currently, this is simplified in \nowdb.
The \term{where} clause is indeed a Boolean expression,
but the leaves of those expressions are always
comparisons of the form \identifier{field} = \term{value}.
It is currently not possible to compare fields with fields
or values with values; it is also not possible to say,
for instance,
\keyword{where} \keyword{true}.}

A very simple \term{where} clause would be:

\keyword{where} \identifier{prod\_key} = 100001

Which evaluates to true for all rows that have $100001$
as \identifier{prod\_key}.

Since \term{where} clauses are Boolean expressions,
it is possible to use the Boolean operators
\keyword{and}, \keyword{or} and \keyword{not}.
The first two operators are binary, \ie\
they expect two operands (which again are
Boolean expressions), while \keyword{not}
is unary and expects one operand (which again
is a Boolean expression).

The Boolean operators combine comparisons
with each other. Comparison operators are
$=, <, >, \le, \ge, \ne$.
There are two forms to express the $\ne$
operator, namely ``$<>$'' and ``!=''. 

For instance:

\keyword{where} \identifier{destin} = 100001
\keyword{and} \identifier{timestamp} $\le$ '2018-04-01'

Before we look at more interesting cases,
let's assume the following set of data:

\begin{minipage}{\textwidth}
\begin{verbatim}
|---------+--------------|
| destin  | timestamp    |
|---------+--------------|
| 100001  | '2018-03-15' |
| 100002  | '2018-03-15' |
| 100001  | '2018-04-15' |
| 100002  | '2018-04-15' |
| 100001  | '2018-05-15' |
| 100002  | '2018-05-15' |
| 100001  | '2018-06-15' |
| 100002  | '2018-06-15' |
|----------+-------------|
\end{verbatim}
\end{minipage}

Here is a tricky \keyword{where} clause:

\keyword{where} \identifier{destin} = 100001
\keyword{and} \identifier{timestamp} $<$ '2018-04-01'
\keyword{or} \identifier{timestamp} $\ge$ '2018-05-01'

\keyword{or} has precedence over \keyword{and}.
Both, \keyword{or} and \keyword{and} bind to the left,
\ie\ the first operand is the one in front of the operator.

That means, here, that \keyword{or} is at the top-level:
The clause selects all rows that have \keyword{timestamp}
$\ge$ May, 1, and those that have \keyword{destination} $100001$
and \keyword{timestamp} less than April, 1.
In other words, what we see is

\begin{minipage}{\textwidth}
\begin{verbatim}
|---------+--------------|
| destin  | timestamp    |
|---------+--------------|
| 100001  | '2018-03-15' |
| 100001  | '2018-05-15' |
| 100002  | '2018-05-15' |
| 100001  | '2018-06-15' |
| 100002  | '2018-06-15' |
|----------+-------------|
\end{verbatim}
\end{minipage}

This might be surprising at the first glance.
But, indeed, most \sql\ dialects follow this convention.

To force another binding, parentheses can be used:

\begin{minipage}{\textwidth}
\keyword{where} \keyword{destin} = 100001 \\
\hspace*{0.45cm}\keyword{and} (\keyword{timestamp} $<$ '2018-04-01'
\keyword{or} \keyword{timestamp} $\ge$ '2018-05-01')
\end{minipage}

This clause would now select all rows
that have \keyword{destination} $100001$ and
a \keyword{timestamp} that is not in April,
like this:

\begin{minipage}{\textwidth}
\begin{verbatim}
|---------+--------------|
| destin  | timestamp    |
|---------+--------------|
| 100001  | '2018-03-15' |
| 100001  | '2018-05-15' |
| 100001  | '2018-06-15' |
|----------+-------------|
\end{verbatim}
\end{minipage}

Equivalent to this second clause is

\keyword{where} \identifier{destin} = 100001 \\
\hspace*{0.45cm}\keyword{and} \keyword{not} 
(\keyword{timestamp} $\ge$ '2018-04-01'
\keyword{and} \keyword{timestamp} $<$ '2018-05-01')

\keyword{or} and \keyword{and} have precedence
over \keyword{not}; \keyword{not} binds to the right,
\ie\ \keyword{not} is a \term{prefix}.

Would we again leave out the parentheses,
like this:

\keyword{where} \identifier{destin} = 100001 \\
\hspace*{0.45cm}\keyword{and} \keyword{not} 
\keyword{timestamp} $\ge$ '2018-04-01'
\keyword{and} \keyword{timestamp} $<$ '2018-05-01'

we would select all rows that have \keyword{destination}
$100001$ and not a \keyword{timestamp} greater April
and that are before May, hence:

\begin{minipage}{\textwidth}
\begin{verbatim}
|---------+--------------|
| destin  | timestamp    |
|---------+--------------|
| 100001  | '2018-03-15' |
|----------+-------------|
\end{verbatim}
\end{minipage}

Besides the basic comparison operators
there are some more very useful operators,
namely
\keyword{between},
\keyword{in},
\keyword{having},
\keyword{exists} and
\keyword{any} and \keyword{all}.

\subsubsection{between}
\comment{Not yet available}

\subsubsection{in}
\comment{Not yet available}

\subsubsection{having}
\comment{Not yet available}

\subsubsection{exists}
\comment{Not yet available}

\subsubsection{any and all}
\comment{Not yet available}

\subsection{While Clause}
The \term{while} clause implements a common requirement
in graph databases, namely to follow a link recursively.
A typical example is social network where we want to
know if $A$ is connected to $B$ through a path spanning
more than one edge (of the same kind).
Here is an illustration:

\begin{minipage}{\textwidth}
\keyword{select} \keyword{true}
\keyword{from} \identifier{friend} \\
\keyword{where} \keyword{edge} $=$ 'isfriend' \\
\keyword{while} \keyword{destin} $\neq$ 12345 
\end{minipage}

This statement would follow the ``isfriend'' edges
until an edge with destination $12345$ is found.

\comment{
This is not yet available -- and there are some
aspects to clarify: First, the number of iterations
must be bounded; otherwise we may follow a
cycle for eternity. Second, this, obviously, works
only for edges where origin and destination
have the same type. Interesting
would be cases where the types of origin and
destination alternate, \eg\
\identifier{client} `buys' \identifier{product} and
\identifier{product} \keyword{passive}(`buys') \identifier{client}.
The `operator' \term{passive} here would turn the edge around:
we first go from \identifier{client} to \identifier{product}
and then from \identifier{product} to (another) \identifier{client}.
With this, however, a new difficulty arises:
how to avoid the combinatorial explosion after some,
in fact very few, iterations?
More efficient for such cases is probably just to loop
through all edges.
}

\subsection{Group Clause}\label{sec_group}
Grouping partitions the result set
according to a set of \term{keys}.
The result set will then be presented
according to these partitions.
The keys used to partition the set
are defined in the \term{group} clause.
A simple \term{group} clause could be:

\keyword{group by} \keyword{edge, destin}

This clause would partition the result
according to \keyword{ege} and \keyword{destin}.

In the simplest from a statement with
group statement could look like this:

\keyword{select} \keyword{edge},
                 \keyword{destin}
\keyword{from} \identifier{sales}
\keyword{group by} \keyword{edge, destin}

Let's consider the following set of data

\begin{minipage}{\textwidth}
\begin{verbatim}
|----------+---------+--------------|
| edge     | destin  | timestamp    |
|----------+---------+--------------|
| 'buys'   | 100001  | '2018-03-15' |
| 'buys'   | 100002  | '2018-03-15' |
| 'buys'   | 100001  | '2018-04-15' |
| 'buys'   | 100002  | '2018-04-15' |
| 'buys'   | 100001  | '2018-05-15' |
| 'buys'   | 100002  | '2018-05-15' |
| 'buys'   | 100001  | '2018-06-15' |
| 'steals' | 100002  | '2018-06-15' |
|----------+----------+-------------|
\end{verbatim}
\end{minipage}

Applied to this set,
the group above would produce the following output

\begin{minipage}{\textwidth}
\begin{verbatim}
|----------+---------|
| edge     | destin  |
|----------+---------|
| 'buys'   | 100001  |
| 'buys'   | 100002  |
| 'steals' | 100002  |
|----------+---------|
\end{verbatim}
\end{minipage}

It, hence, produces a distinct set of the key values.
One could say that grouping abstracts or reduces 
the data set according to the keys.
This, in itself is often a useful feature,
\viz\ when we want to apply a certain processing
only once per key. As an aside it may be mentioned
that this kind of grouping is extremely fast in \nowdb;
for performance considerations in general, please
refer to chapter \ref{chpt_opt}.

The real power of grouping becomes evident
when we let the database do the processing.
This is the purpose of \term{aggregate functions}.
Aggregates produce a result per group (\ie\ per partition).
Aggregates are for example
\term{count}, \term{sum},
\term{max} and \term{min} or \term{avg}.
A complete list of aggregates supported in \nowdb\
can be found below in sections \ref{sec_agg}.

The aggregate function goes to the \term{select} clause
behind the grouping keys:

\keyword{select} \keyword{edge},
                 \keyword{destin},
                 \keyword{count}(*)
\keyword{from} \identifier{sales}
\keyword{group by} \keyword{edge, destin}

This would now produce:

\begin{minipage}{\textwidth}
\begin{verbatim}
|----------+---------+-------|
| edge     | destin  | count |
|----------+---------|-------|
| 'buys'   | 100001  |  4    |
| 'buys'   | 100002  |  3    |
| 'steals' | 100002  |  1    |
|----------+---------+-------|
\end{verbatim}
\end{minipage}

The \term{group} clause has strong interdependencies
with the \term{select} clause.
In particular, the \term{select} clause must
contain precisely those fields mentioned in the
\term{group} clause and the order of the fields
must be identical. The only things allowed in
the \term{select} clause besides the grouping keys
are aggregates.

The following statements are therefore wrong:

\keyword{select} \keyword{edge}, 
                 \keyword{count}(*)
\keyword{from} \identifier{sales}
\keyword{group by} \keyword{edge, destin}

\keyword{select} \keyword{destin},
                 \keyword{edge}, 
                 \keyword{count}(*)
\keyword{from} \identifier{sales}
\keyword{group by} \keyword{edge, destin}

\keyword{select} \keyword{edge}, 
                 \keyword{destin},
                 'hello world'
\keyword{from} \identifier{sales}
\keyword{group by} \keyword{edge, destin}

Notice that aggregates without grouping
are applied on the whole result set.
For instance, to count all rows in \identifier{sales},
one could say:

\keyword{select} \keyword{count}(*)
\keyword{from} \identifier{sales}

The partition on which \keyword{count} is applied here
is the whole result set.

Syntactically, even this is \acronym{ok}:

\keyword{select} \keyword{edge},
                 \keyword{destin},
                 \keyword{count}(*)
\keyword{from} \identifier{sales}

The values shown for \keyword{edge} and \keyword{destin},
however, have no relation whatsoever to the \keyword{count}
result. They correspond
to a any of the rows in the result set,
the first, the last or any other.
That is unspecified.

\comment{
To be discussed: grouping and having.
}

\comment{
Currently, grouping is only possible for combinations of keys
for which an index exists. That is to say:
the index must be defined over the same set of keys
and the order of the keys must be the same.
Otherwise, when there is not matching index,
the query fails with error.
I do not consider this a huge issue.
Grouping is an expensive operation and if,
in an application, grouping is frequently needed,
it should be considered in database design.
However, there a some cases where I am concerned
about performance. In general, the performance
is good or even fantastic. But there are cases
that could motivate alternative grouping techniques.
}

\subsection{Order Clause}
The \term{order} clause sorts the result set
according to a set of sorting criteria (the \term{order keys}).
Its simplest form is just:

\keyword{order by} \keyword{edge}, \keyword{destin}

which would present the result set (whatever it contains)
sorted by \keyword{edge} and \keyword{destin}.

\begin{minipage}{\textwidth}
The statement

\keyword{select} \keyword{edge}, \keyword{destin}
\keyword{from} \identifier{sales}
\keyword{order by} \keyword{edge}, \keyword{destin}
\end{minipage}

applied to the data set already used in section
\ref{sec_group} would produce the following output:

\begin{minipage}{\textwidth}
\begin{verbatim}
|----------+---------+--------------|
| edge     | destin  | timestamp    |
|----------+---------+--------------|
| 'buys'   | 100001  | '2018-03-15' |
| 'buys'   | 100001  | '2018-04-15' |
| 'buys'   | 100001  | '2018-05-15' |
| 'buys'   | 100001  | '2018-06-15' |
| 'buys'   | 100002  | '2018-03-15' |
| 'buys'   | 100002  | '2018-04-15' |
| 'buys'   | 100002  | '2018-05-15' |
| 'steals' | 100002  | '2018-06-15' |
|----------+----------+-------------|
\end{verbatim}
\end{minipage}

Performance considerations are important when dealing with sorting.
It is therefore recommended to study section \ref{chpt_opt} with care.

\comment{
Similar to grouping, ordering works only with an index
that has the same keys in the same order.
Contrary to the grouping case, for ordering this is not acceptable.
Ordering is needed over and over again and good sorting
algorithms must be available in the database.
Well, the algorithms are there (index sorting, in-memory sorting and
external sorting). The question is: how to apply them in
the most efficient manner.
For instance, index sorting is good when we have a range,
otherwise it's bad.
In-memory sorting is good when the result set is small,
otherwise it's bad.
External sorting is always bad, but the last resort.
A special case is ordering by timestamp.
In a timeseries database, that should always be
available!!!
The solution I have in mind is to always create
an index on timestamp with a micro-period (like 1 hour or
1 minute). The rows within one micro-period would be
sorted in memory and, since there are few rows in one
micro-period that is fast and does not impact latency.
The whole query may be slow, but there are always
data for the caller to make progress.
}

\subsection{Operators and Functions}
\subsubsection{Boolean}
$=,<,>,\le,\ge,\neq$,
\term{between},
\term{and}, \term{or}, \term{not},
\term{in}, \term{any}, \term{all},
\term{exists}

\subsubsection{Arithmetic}
$+,-,\times, \div, \%, m^n, \sqrt[m]{n}$,
$\log, \mod, \dots$

\comment{Not yet available}

\subsubsection{Text}
\comment{Not yet available}

\subsection{Aggregates}\label{sec_agg}
\subsubsection{count}
The function is applied either to the row (\keyword{count}(*))
or to a field (\keyword{count}(\identifier{field}).
Applied to the entire row, it counts the number of elements
in the result set.
When applied to a field, it is usually combined with
\keyword{distinct} (\comment{which is not yet available}),
otherwise the result would be the same as applying it to the row.
Common examples are therefore:

\keyword{select} \keyword{count}(*) \keyword{from} \identifier{sales}

and

\keyword{select} \keyword{count}(\keyword{distinct} \keyword{edge})
\keyword{from} \identifier{sales}

\comment{
In the following sections, we leave the option to qualify
a field with \keyword{distinct} out (as well as any other
qualifiers). It is not yet available anyway.
}

\subsubsection{sum}
The function is always a applied to a field.
and produces the sum of the values in that field over the result set.
Example:

\keyword{select} \keyword{sum}(\keyword{weight}) \keyword{from} \identifier{sales}

\subsubsection{max and min}
The functions are always applied to a field.
Function \term{max} produces the greatest value in the result set
and function \term{min} produces the smallest value in the result set.
Example:

\keyword{select} \keyword{max}(\keyword{weight}), \keyword{min}(\keyword{weight})
\keyword{from} \identifier{sales}

\subsubsection{spread}
The Function is always applied to a field.
It produces the difference $max - min$,
where $max$ is the greatest value in the result set
and $min$ is the smallest value in the result set.
Example:

\keyword{select} \keyword{spread}(\keyword{weight})
\keyword{from} \identifier{sales}

\subsubsection{avg}
The Function is always applied to a field.
It produces the arithmetic mean of the values
of this field in the result set, \ie
$\left(\sum{x}\right)/n$, where $x$ represents the
field values in the result set and $n$
is the number of rows (\ie\ \keyword{count}(*)).
Example:

\keyword{select} \keyword{avg}(\keyword{weight})
\keyword{from} \identifier{sales}

\subsubsection{median}
The function is always applied to a field.
It finds the central point (or, in case
the number of rows is even, the average
of the two central points), when all
rows are ordered according to the field.
Example:

\keyword{select} \keyword{median}(\keyword{weight})
\keyword{from} \identifier{sales}

\comment{
No precaution is currently taken for the case
that the result set outgrows available memory.
In that case the query will fail (and probably the queries
in other sessions too).
}

\subsubsection{mode}
The function is always applied to a field.
It finds the most frequent value in the result set.

\keyword{select} \keyword{mode}(\keyword{weight})
\keyword{from} \identifier{sales}

\comment{
Not yet available
}

\subsubsection{stddev}
The function is always applied to a field.
It finds the standard deviation according to the formula:

\[
\sqrt{\frac{\sum_{i=N}^{N}{(x_i - \overline{x})^2}}{N-1}},
\]

where $N$ is the number of rows in the result set,
$x_i$ is the $i$th element and $\overline{x}$ is
the arithmetic mean (\ie\ \keyword{avg}).

\keyword{select} \keyword{stddev}(\keyword{weight})
\keyword{from} \identifier{sales}

\comment{
Note that there is not much optimisation done in aggregates.
For instance, \term{median} and \term{stddev} create a
collection of all values, but each one creates its own collection.
That is, the values are collected twice.
}

\subsubsection{integral}
The function is always applied to a field and
can only be applied to edges.
It computes the area under the curve with
\term{timestamp} on the $x$-axis and the field,
to which \term{integral} is applied,
on the $y$-axis. Example:

\keyword{select} \keyword{integral}(\keyword{weight})
\keyword{from} \identifier{sales}

\comment{
Not yet available
}

\section{Miscellaneous}
\subsection{Use}
The \term{use} statement sets the database for
the current session. All following statements
 until the next \term{use} statement
will be applied to this database.
Example:

\keyword{use} \identifier{retail}

The statement returns a status result.

\subsection{Exec}
The \term{exec} statement executes a stored procedure
whose interface was previously defined.
The simplest form is

\keyword{exec} \identifier{myprocedure}()

In this case \identifier{myprocedure} has no parameters.
A concrete example with parameters may be

\keyword{exec} \identifier{revenue}(9000001, '2018-05-01')

The result of the statement depends on the procedure.
